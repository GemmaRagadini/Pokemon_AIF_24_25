{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaRagadini/Pokemon_AIF_24_25/blob/dev/pokemon-vgc-engine-master/example/Notebook_project_Pokemon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pokemon_Battle PokeBob team**\n",
        "\n",
        "The project focuses on the track proposed in the course, specifically the section related to competitions. Our team selected the task involving the simulation of a Pokémon battle between two teams, each composed of three Pokémon. The battle consists of three matches, with the first player to knock out all three Pokémon of the opposing player declared the winner of the match. The battle is considered concluded when a player wins at least two out of three matches.\n",
        "\n",
        "The objective of the project was to develop an AI agent capable of defeating a random player. This random player selects its Pokémon moves arbitrarily without any strategic logic. Both players operate under the same conditions, with their teams assigned randomly, ensuring that any advantages or disadvantages are also determined by chance\n",
        "In the following sections, we will explain our approach to solving the task and outline the methodology we adopted, along with the results we obtained.\n"
      ],
      "metadata": {
        "id": "uSQ9kvX2QiD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone https://github.com/GemmaRagadini/Pokemon_AIF_24_25.git\n",
        "%cd Pokemon_AIF_24_25/pokemon-vgc-engine-master\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE_4RJnMlLfh",
        "outputId": "d566b164-1b5a-497f-d458-1e5772ba2fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pokemon_AIF_24_25'...\n",
            "remote: Enumerating objects: 1863, done.\u001b[K\n",
            "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (172/172), done.\u001b[K\n",
            "remote: Total 1863 (delta 195), reused 125 (delta 77), pack-reused 1614 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1863/1863), 21.61 MiB | 18.54 MiB/s, done.\n",
            "Resolving deltas: 100% (417/417), done.\n",
            "/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/Pokemon_AIF_24_25/pokemon-vgc-engine-master/vgc/Pokemon_AIF_24_25/pokemon-vgc-engine-master/vgc/behaviour/Pokemon_AIF_24_25/pokemon-vgc-engine-master/Pokemon_AIF_24_25/pokemon-vgc-engine-master/Pokemon_AIF_24_25/pokemon-vgc-engine-master\n",
            "CHANGELOG    example\t  organization\trequirements.txt  test\n",
            "competition  LICENSE.txt  README.md\tsetup.py\t  vgc\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement requirements.txt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0mHINT: You are attempting to install a package literally named \"requirements.txt\" (which cannot exist). Consider using the '-r' flag to install the packages listed in requirements.txt\n",
            "\u001b[31mERROR: No matching distribution found for requirements.txt\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "2yvr8UnLOXeR",
        "outputId": "6c3fd63f-6fee-46aa-d627-2870624134e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vgc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2a6e9b6dce07>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from Example_BattleEcosystem import Tournament, main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mExample_BattleEcosystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/example/Example_BattleEcosystem.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mExample_Competitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExampleCompetitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardMetaData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompetitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompetitorManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mecosystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBattleEcosystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBattleEcosystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPkmRosterGenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomPkmRosterGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/example/Example_Competitor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBattlePolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTeamSelectionPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTeamBuildPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBattlePolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomPlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTerminalPlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTeamBuildPolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTerminalTeamBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomTeamBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTeamSelectionPolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFirstEditionTeamSelectionPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompetitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompetitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vgc'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "#from Example_BattleEcosystem import Tournament, main\n",
        "from Example_BattleEcosystem.py import main\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Related Work**\n",
        "\n",
        "For this project, we decided to explore some existing approaches related to the concepts studied during the course, as well as to develop a custom approach of our own. These approaches were evaluated through a tournament, the details of which will be provided in the corresponding section.\n",
        "\n",
        "The related work was derived from the course slides and the accompanying textbook, Artificial Intelligence Fundamentals. Specifically, we focused on the section related to game theory (Chapter 6 of the textbook) and examined various approaches to the Minimax algorithm and its variations. This included the implementation of alpha-beta pruning and heuristics such as the killer move heuristic."
      ],
      "metadata": {
        "id": "czJQB6UsZVMv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jU6PcE_DhArV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methodology**\n",
        "\n",
        "To achieve our goal, we decided to implement several algorithms discussed in the related work. Initially, we focused our attention on various implementations of the Minimax algorithm. In the following sections, we will provide a detailed explanation of each algorithm we implemented."
      ],
      "metadata": {
        "id": "RLuFhPYL_ZQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List\n",
        "from vgc.behaviour import evalFunctions\n",
        "from vgc.datatypes.Types import PkmStat, PkmType, WeatherCondition\n",
        "from vgc.datatypes.Objects import Pkm, GameState,PkmType\n",
        "from vgc.datatypes.Constants import DEFAULT_PKM_N_MOVES, DEFAULT_PARTY_SIZE, TYPE_CHART_MULTIPLIER, DEFAULT_N_ACTIONS\n",
        "from vgc.behaviour import BattlePolicy\n",
        "from copy import deepcopy\n",
        "\n",
        "#import that are useful to run our code for the implementattion of all the policies"
      ],
      "metadata": {
        "id": "uiuBcrxL7hDO",
        "outputId": "51096114-06e6-424c-8481-322b93c63f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'evalFunctions' from 'vgc.behaviour' (/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/vgc/behaviour/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-34b0082c33e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevalFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPkmStat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPkmType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeatherCondition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjects\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPkm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGameState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPkmType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'evalFunctions' from 'vgc.behaviour' (/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/vgc/behaviour/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minimax**\n",
        "\n",
        "The first implementation is the one obout a simple minimax with a simple evaluation function called game_eval. Here below there is our implementation.\n"
      ],
      "metadata": {
        "id": "71GjO_fpBQUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def game_state_eval(s: GameState, depth):\n",
        "    mine = s.teams[0].active\n",
        "    opp = s.teams[1].active\n",
        "    return mine.hp / mine.max_hp - 3 * opp.hp / opp.max_hp - 0.3 * depth\n",
        "\n",
        "\n",
        "def n_fainted(t: PkmTeam):\n",
        "    fainted = 0\n",
        "    fainted += t.active.hp == 0\n",
        "    if len(t.party) > 0:\n",
        "        fainted += t.party[0].hp == 0\n",
        "    if len(t.party) > 1:\n",
        "        fainted += t.party[1].hp == 0\n",
        "    return fainted\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyMinimax(BattlePolicy):\n",
        "\n",
        "    def __init__(self, max_depth: int = 4):\n",
        "        self.max_depth = max_depth\n",
        "        self.name = \"Minimax\"\n",
        "\n",
        "    def minimax(self, g, depth, is_maximizing_player):\n",
        "        \"\"\"\n",
        "        Classic Minimax with basic evaluation function.\n",
        "\n",
        "        :param g: current game state.\n",
        "        :param depth: the depth of the research.\n",
        "        :param is_maximizing_player: True if the player is a maximaxer or False if it is a minimazer.\n",
        "        :return: (valutazione, azione migliore)\n",
        "        \"\"\"\n",
        "        if depth == 0:\n",
        "            # the evaluation function is the basic evaluation function that evaluates the Hp of the pokemons.\n",
        "            return game_state_eval(g, depth), None\n",
        "\n",
        "        if is_maximizing_player:\n",
        "            max_eval = float('-inf')\n",
        "            best_action = None\n",
        "            for i in range(DEFAULT_N_ACTIONS):\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([i, 99])  # the enemy does not do a correct action the state does not change\n",
        "                if n_fainted(s[0].teams[0]) > n_fainted(g.teams[0]):\n",
        "                    continue # ignores the state where our pokemon loose.\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, False)\n",
        "                if eval_score > max_eval:\n",
        "                    max_eval = eval_score\n",
        "                    best_action = i\n",
        "            return max_eval, best_action\n",
        "\n",
        "        else:  # part of the enemy where he tries to minimize\n",
        "            min_eval = float('inf')\n",
        "            best_action = None\n",
        "            for j in range(DEFAULT_N_ACTIONS):\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([99, j])  # The player does not change the action (not valid action)\n",
        "                # it ignores the state where the defeated pokemon of the enemy increase.\n",
        "                if n_fainted(s[0].teams[1]) > n_fainted(g.teams[1]):\n",
        "                    continue\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, True)\n",
        "                if eval_score < min_eval:\n",
        "                    min_eval = eval_score\n",
        "                    best_action = j\n",
        "            return min_eval, best_action\n",
        "\n",
        "    def get_action(self, g) -> int:\n",
        "        \"\"\"\n",
        "        bets action to do by the maximazer player.\n",
        "\n",
        "        :param g: current game state.\n",
        "        :return: bets action to do.\n",
        "        \"\"\"\n",
        "        _, best_action = self.minimax(g, self.max_depth, True)\n",
        "        return best_action if best_action is not None else 0\n",
        "\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, state, parent, player, action=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.player = player\n",
        "        self.action = action\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.value = 0\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.get_untried_actions()) == 0\n",
        "\n",
        "    def get_untried_actions(self):\n",
        "        # Ottiene le azioni possibili dallo stato\n",
        "        return [i for i in range(DEFAULT_N_ACTIONS)]\n"
      ],
      "metadata": {
        "id": "WtNN0DTFC2Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell above, we present our first implementation of the Minimax policy, which employs a basic evaluation function called game_state_eval. This function is designed to:\n",
        "\n",
        "Encourage states where the player’s active Pokémon (mine) has higher HP relative to its maximum HP.\n",
        "Penalize states where the opponent’s active Pokémon (opp) has high HP.\n",
        "Add a penalty proportional to the search depth to prioritize faster victories.\n",
        "Although this evaluation function is rudimentary, our results demonstrate that it provides a balanced implementation. However, it is not the most effective approach we encountered.\n",
        "\n",
        "The Minimax implementation is straightforward, comprising a section for the maximizer player and another for the minimizer. The maximizer aims to transition to states where its Pokémon are healthier than the opponent’s Pokémon, while the minimizer seeks to reduce this advantage. Each state is evaluated recursively.\n",
        "\n",
        "To support these computations, the algorithm uses the n_fainted function, which counts the number of fainted (knocked-out) Pokémon. Additionally, the algorithm determines the next action from the maximizer player’s perspective, as implemented in the get_action method.\n",
        "\n",
        "Another critical function used is the step function, which simulates actions to predict how the game state evolves. This function works in conjunction with the n_fainted function to enhance decision-making.\n",
        "\n",
        "The default values for the search depth and weights in the evaluation function were determined empirically. Various configurations were tested, and the ones used here were found to deliver the best performance according to our evaluation metrics.\n",
        "\n",
        "The Node class, shown at the end of the cell, represents a node in the Minimax tree. It includes several fields to facilitate tree exploration, such as parent, children, value, and state.\n",
        "\n",
        "Additionally, the class provides two key methods:\n",
        "\n",
        "fully_expand: Checks whether the node has been fully explored.\n",
        "get_untried_actions: Retrieves the set of possible actions that can still be taken from the current node.\n",
        "These fields and methods are critical for efficiently navigating and expanding the Minimax tree during the decision-making process.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TX6rZ_AqEoUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minimax with Alpha-Beta Pruning and Killer Move Heuristic**\n",
        "\n",
        "Our second implementation extends the basic Minimax algorithm by incorporating alpha-beta pruning and the killer move heuristic. This implementation was developed to enhance both the performance and efficiency of the Minimax algorithm described above.\n",
        "\n",
        "The addition of alpha-beta pruning allows the algorithm to eliminate branches in the search tree that cannot influence the final decision, significantly reducing the number of nodes explored. Meanwhile, the killer move heuristic prioritizes moves that are likely to be effective, further optimizing the decision-making process by focusing on promising actions.\n",
        "\n",
        "The combined use of these techniques aims to not only improve the accuracy of the algorithm but also speed up its execution, enabling faster and more effective decision-making.\n",
        "\n",
        "In the cells below, we present our implementation of this enhanced Minimax algorithm along with a detailed explanation of how it operates."
      ],
      "metadata": {
        "id": "mk6j3adLRxU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMinimaxWithAlphaBetaKiller(BattlePolicy):\n",
        "\n",
        "    def __init__(self, max_depth: int = 5):\n",
        "        self.max_depth = max_depth\n",
        "        self.name = \"Minimax with pruning alpha beta killer\"\n",
        "        self.killer_moves = {depth: [] for depth in range(max_depth + 1)}  # Memorizza le killer moves per profondità\n",
        "\n",
        "    def minimax(self, g, depth, alpha, beta, is_maximizing_player):\n",
        "        if depth == 0:\n",
        "            return evalFunctions.game_state_eval(g, depth), None\n",
        "\n",
        "        if is_maximizing_player:\n",
        "            max_eval = float('-inf')\n",
        "            best_action = None\n",
        "\n",
        "            # Ottieni le azioni disponibili\n",
        "            moves = list(range(DEFAULT_N_ACTIONS))\n",
        "\n",
        "            # Prioritizza le killer moves\n",
        "            killer_moves = self.killer_moves.get(depth, [])\n",
        "            moves = sorted(moves, key=lambda move: move in killer_moves, reverse=True)\n",
        "\n",
        "            for i in moves:\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([i, 99])\n",
        "                if evalFunctions.n_fainted(s[0].teams[0]) > evalFunctions.n_fainted(g.teams[0]):\n",
        "                    continue\n",
        "\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, alpha, beta, False)\n",
        "                if eval_score > max_eval:\n",
        "                    max_eval = eval_score\n",
        "                    best_action = i\n",
        "\n",
        "                alpha = max(alpha, eval_score)\n",
        "                if beta <= alpha:\n",
        "                    # Aggiorna le killer moves\n",
        "                    if i not in self.killer_moves[depth]:\n",
        "                        self.killer_moves[depth].append(i)\n",
        "                        if len(self.killer_moves[depth]) > 2:\n",
        "                            self.killer_moves[depth].pop(0)\n",
        "                    break\n",
        "            return max_eval, best_action\n",
        "\n",
        "        else:\n",
        "            min_eval = float('inf')\n",
        "            best_action = None\n",
        "\n",
        "            # Ottieni le azioni disponibili\n",
        "            moves = list(range(DEFAULT_N_ACTIONS))\n",
        "\n",
        "            # Prioritizza le killer moves\n",
        "            killer_moves = self.killer_moves.get(depth, [])\n",
        "            moves = sorted(moves, key=lambda move: move in killer_moves, reverse=True)\n",
        "\n",
        "            for j in moves:\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([99, j])\n",
        "                if evalFunctions.n_fainted(s[0].teams[1]) > evalFunctions.n_fainted(g.teams[1]):\n",
        "                    continue\n",
        "\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, alpha, beta, True)\n",
        "                if eval_score < min_eval:\n",
        "                    min_eval = eval_score\n",
        "                    best_action = j\n",
        "\n",
        "                beta = min(beta, eval_score)\n",
        "                if beta <= alpha:\n",
        "                    # Aggiorna le killer moves\n",
        "                    if j not in self.killer_moves[depth]:\n",
        "                        self.killer_moves[depth].append(j)\n",
        "                        if len(self.killer_moves[depth]) > 2:\n",
        "                            self.killer_moves[depth].pop(0)\n",
        "                    break\n",
        "            return min_eval, best_action\n",
        "\n",
        "    def get_action(self, g) -> int:\n",
        "        _, best_action = self.minimax(g, self.max_depth, float('-inf'), float('inf'), True)\n",
        "        return best_action if best_action is not None else 0"
      ],
      "metadata": {
        "id": "nRhf_-spRwrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we illustrate the key components of this algorithm.\n",
        "\n",
        "Alpha-Beta Pruning is a fundamental optimization technique used in Minimax algorithms as we said before. It reduces the number of nodes evaluated in the game tree by eliminating branches that cannot influence the final decision. Specifically:\n",
        "Alpha represents the best score achievable by the maximizing player, ensuring the current node's value is not less than this threshold.\n",
        "Beta represents the best score achievable by the minimizing player, ensuring the current node's value does not exceed this threshold. By comparing node evaluations against these bounds, the algorithm can skip unnecessary evaluations and focus on the most promising branches.\n",
        "The Killer Move Heuristics prioritize actions (or moves) that previously led to a cutoff during Alpha-Beta Pruning at the same depth. These \"killer moves\" are stored in a depth-specific list and revisited with high priority in subsequent iterations. The rationale is that actions causing significant pruning in similar scenarios are likely to be effective again, reducing the time spent on less impactful moves.\n",
        "\n",
        "The algorithm explores the game tree up to a predefined depth, *max_depth*. At this point, it evaluates the game state using a heuristic evaluation function, at the moment the same of the latest implementation of Minimax. This depth limit balances the trade-off between computational feasibility and decision quality and it was selected in a empirical way.\n",
        "\n",
        "Algorithm Workflow\n",
        "\n",
        "The algorithm begins by initializing necessary parameters, including the maximum search depth and a set of \"killer moves\" for each depth level. These killer moves are essentially a list of actions that have proven effective in causing cutoffs during previous searches. By storing these moves, the algorithm can prioritize their exploration in subsequent iterations, aiming to reduce unnecessary computations.\n",
        "\n",
        "Once initialized, the algorithm proceeds to explore the game tree using the Minimax method. This exploration is depth-limited, meaning the algorithm only evaluates the game tree to a specific depth (*max_depth*) to keep the computation manageable. At the root node, the algorithm decides whether it is currently the turn of the maximizing or minimizing player and selects actions accordingly.\n",
        "\n",
        "For the maximizing player, the goal is to find the action that yields the highest evaluation score and where the number of fainted pokemon is higher for the opponent, representing the most advantageous outcome. Conversely, for the minimizing player, the focus is on selecting the action that minimizes the evaluation score, simulating an opponent trying to counteract the maximizing player’s strategies. At each level of the game tree, Alpha and Beta values are used to dynamically track the best and worst outcomes possible for the respective players. These values guide the pruning process, helping the algorithm decide when to stop exploring certain branches.\n",
        "\n",
        "The algorithm introduces a prioritization mechanism at this stage by sorting available actions based on the killer moves. If a move caused a cutoff at the same depth in a previous search, it is considered likely to do so again and is explored first. This ensures the algorithm focuses on the most promising actions early on, increasing the chances of quickly achieving cutoffs. For example, if the algorithm identifies a move that significantly improves the maximizing player’s position, it will immediately consider pruning all remaining moves that cannot surpass this outcome.\n",
        "\n",
        "As the game tree is traversed, the algorithm continuously updates the Alpha and Beta values. When a node’s evaluation score falls outside the range defined by Alpha and Beta, the algorithm terminates further exploration of that branch. This process, known as pruning, saves computational resources by avoiding the evaluation of irrelevant or unpromising paths. If a cutoff occurs, the current move is added to the list of killer moves for the corresponding depth, ensuring that it will be prioritized in future iterations.\n",
        "\n",
        "Once all possible actions are evaluated, the algorithm determines the best action for the maximizing player at the root node."
      ],
      "metadata": {
        "id": "i8hRaI0RkecE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Evaluation Function**\n",
        "\n",
        "We decided to implement an other evaluation function that differs in the approch respect to the one described above. This new evaluation function is a more aggressive evaluation function in fact take into consideration only the power and the effectivness of a move against the opponent. The aim of this new evaluation function was to create a more aggresive agent so the battles could and in more rapid way. *The result obtained are illustrated and commented in the Evaluation section*."
      ],
      "metadata": {
        "id": "Wh7c3eUhqjAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_eval_fun(s:GameState, depth):\n",
        "    \"\"\"\n",
        "    Funzione di valutazione che considera la compatibilità tra i pokemon, la game_state_eval rispetto agli hp e\n",
        "    la possibilità di infliggere danno\n",
        "    \"\"\"\n",
        "    my_active = s.teams[0].active\n",
        "    opp_active = s.teams[1].active\n",
        "    attack_stage = s.teams[0].stage[PkmStat.ATTACK]\n",
        "    defense_stage = s.teams[1].stage[PkmStat.DEFENSE]\n",
        "    matchup = evaluate_matchup(my_active.type, opp_active.type, list(map(lambda m: m.type, my_active.moves))) # in [0,2]\n",
        "    eval_hp = game_state_eval(s,depth) + 4 # circa in [0-5]\n",
        "    max_damage = maxDamage(my_active, opp_active.type, attack_stage, defense_stage, s.weather) # in [0,140]\n",
        "    return max_damage/70 + matchup/2 + eval_hp\n",
        "\n",
        "\n",
        "def maxDamage(my_active: Pkm, opp_active_type:PkmType, attack_stage: int, defense_stage: int,weather: WeatherCondition ):\n",
        "    \"\"\"\n",
        "    Ritorna il massimo danno il pokemon attivo poù infliggere all'avversario con una mossa\n",
        "    \"\"\"\n",
        "    mvs_damage = []\n",
        "    # stimo il danno per ogni mossa del mio pokemon\n",
        "    for m in my_active.moves:\n",
        "        mvs_damage.append(estimate_damage(m.type,my_active.type, m.power, opp_active_type ,attack_stage, defense_stage, weather))\n",
        "    return np.max(mvs_damage)\n",
        "\n",
        "\n",
        "def evaluate_matchup(pkm_type: PkmType, opp_pkm_type: PkmType, moves_type: List[PkmType]) -> float:\n",
        "    \"\"\"\n",
        "    Valuta l'abbinamento tra il pokemon attivo e il pokemon avversario,\n",
        "    considerando i tipi dei pokemon e delle mosse disponibili.\n",
        "    \"\"\"\n",
        "    for mtype in moves_type: # cerca mossa super efficace\n",
        "        if TYPE_CHART_MULTIPLIER[mtype][pkm_type] == 2.0:\n",
        "            return 2.0  # ritorna 2 nel caso in cui ci sia una mossa super efficace\n",
        "    # altrimenti considera solo la valutazione rispetto al tipo di pokemon\n",
        "    return TYPE_CHART_MULTIPLIER[opp_pkm_type][pkm_type]\n"
      ],
      "metadata": {
        "id": "gsw3H7stDlhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Policy**\n",
        "\n",
        "The third and last approch was the one about a our custom Policy. This policy differ form the other that has been implemented"
      ],
      "metadata": {
        "id": "dYhf6x63nQWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPolicy(BattlePolicy):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.hail_used = False\n",
        "        self.sandstorm_used = False\n",
        "        self.name = \"My Policy\"\n",
        "\n",
        "    def estimate_damages(self, active_pkm: Pkm, opp_pkm_type: PkmType, attack_stage: int, defense_stage: int, weather: WeatherCondition)-> int:\n",
        "        # valutazione mosse\n",
        "        damages: List[float] = []\n",
        "        for move in active_pkm.moves:\n",
        "            damages.append(evalFunctions.estimate_damage(move.type, active_pkm.type, move.power, opp_pkm_type, attack_stage,\n",
        "                                          defense_stage, weather))\n",
        "        return damages\n",
        "\n",
        "\n",
        "    def get_action(self, g: GameState) -> int:\n",
        "        # la mia squadra\n",
        "        my_team = g.teams[0]\n",
        "        active_pkm = my_team.active\n",
        "        bench = my_team.party\n",
        "        my_attack_stage = my_team.stage[PkmStat.ATTACK]\n",
        "\n",
        "        # squadra avversaria\n",
        "        opp_team = g.teams[1]\n",
        "        opp_active_pkm = opp_team.active\n",
        "        opp_defense_stage = opp_team.stage[PkmStat.DEFENSE]\n",
        "\n",
        "        # meteo\n",
        "        weather = g.weather.condition\n",
        "\n",
        "        try:\n",
        "            # stima dei danni di ogni mossa\n",
        "            damages = self.estimate_damages(active_pkm, opp_active_pkm.type, my_attack_stage, opp_defense_stage, weather)\n",
        "            move_id = int(np.argmax(damages))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # se elimina l'avversario oppure il tipo di mossa è superefficace si usa subito:\n",
        "        if (damages[move_id] >= opp_active_pkm.hp) or (damages[move_id] > 0 and TYPE_CHART_MULTIPLIER[active_pkm.moves[move_id].type][opp_active_pkm.type] == 2.0) :\n",
        "            return move_id\n",
        "        try:\n",
        "            defense_type_multiplier = evalFunctions.evaluate_matchup(active_pkm.type, opp_active_pkm.type,\n",
        "                                                    list(map(lambda m: m.type, opp_active_pkm.moves)))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        if defense_type_multiplier <= 1.0:\n",
        "            return move_id\n",
        "\n",
        "        # considera il cambio pokemon\n",
        "        matchup: List[float] = []\n",
        "        not_fainted = False\n",
        "\n",
        "        try:\n",
        "            for j in range(len(bench)):\n",
        "                if bench[j].hp == 0.0:\n",
        "                    matchup.append(0.0)\n",
        "                else:\n",
        "                    not_fainted = True\n",
        "                    matchup.append(\n",
        "                        evalFunctions.evaluate_matchup(bench[j].type, opp_active_pkm.type, list(map(lambda m: m.type, bench[j].moves))))\n",
        "\n",
        "            best_switch_matchup = int(np.max(matchup))\n",
        "            best_switch = np.argmax(matchup)\n",
        "            current_matchup = evalFunctions.evaluate_matchup(active_pkm.type, opp_active_pkm.type,list(map(lambda m: m.type, active_pkm.moves)))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        if not_fainted and best_switch_matchup >= current_matchup+1:\n",
        "            return best_switch + 4\n",
        "\n",
        "        return move_id\n"
      ],
      "metadata": {
        "id": "mO64wj0PnP9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descrizione della custom Policy"
      ],
      "metadata": {
        "id": "Z1VhZfhmqNn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of the performances**\n",
        "\n",
        "We implemented several approaches and, in order to determine which policy performed best, we decided to have each agent battle against the Random agent. This allowed us to demonstrate that each policy outperforms the Random agent.\n",
        "\n",
        "For the second evaluation, we organized a tournament involving all the players:\n",
        "\n",
        "*Random Player*\n",
        "\n",
        "*Minimax Player*\n",
        "\n",
        "*Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player*\n",
        "\n",
        "*Custom Policy Player*\n",
        "\n",
        "Each match consisted of 10 battles, and the winner was determined by the player who won the most matches, thus achieving the best win rate. There two types of battle the first 10 battles the two agents involved have always the same team, the second 10 games the two players have two different teams. In all of these cases the team was selected in a random way.\n",
        "\n",
        "The results of each individual battle, as well as the overall winner of the tournament, are reported in the cells below."
      ],
      "metadata": {
        "id": "M43Al93JqRyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time as t\n",
        "from example.Example_Competitor import MyCompetitor0, MyCompetitor1, MyCompetitor2, MyCompetitor3\n",
        "from vgc.balance.meta import StandardMetaData\n",
        "from vgc.competition.Competitor import CompetitorManager\n",
        "from vgc.ecosystem.BattleEcosystem import BattleEcosystem\n",
        "from vgc.util.generator.PkmRosterGenerators import RandomPkmRosterGenerator\n",
        "from vgc.util.generator.PkmTeamGenerators import RandomTeamFromRoster\n",
        "import sys\n",
        "import random\n",
        "#this are the import needed to to run the evaluation of the application"
      ],
      "metadata": {
        "id": "kRLYWhIhIOl0",
        "outputId": "2a7c2669-e76e-43dd-ea0f-ed799f8a34a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'customtkinter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2fa63686f683>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample_Competitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMyCompetitor0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyCompetitor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyCompetitor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyCompetitor3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardMetaData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompetitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompetitorManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mecosystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBattleEcosystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBattleEcosystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/Pokemon_AIF_24_25/pokemon-vgc-engine-master/vgc/Pokemon_AIF_24_25/pokemon-vgc-engine-master/vgc/behaviour/Pokemon_AIF_24_25/pokemon-vgc-engine-master/example/Example_Competitor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBattlePolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTeamSelectionPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTeamBuildPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBattlePolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomPlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTerminalPlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTeamBuildPolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTerminalTeamBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomTeamBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTeamSelectionPolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFirstEditionTeamSelectionPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompetitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompetitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/vgc/behaviour/BattlePolicies.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcustomtkinter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCTkButton\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCTkRadioButton\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCTkLabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBattlePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'customtkinter'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Battle**\n",
        "\n",
        "Minimax vs Random Player: the first result is when the two player have always a different team and the second when they have always the same team"
      ],
      "metadata": {
        "id": "xhi3IXEHsuPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_PLAYERS = 2\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    roster = RandomPkmRosterGenerator().gen_roster()\n",
        "    meta_data = StandardMetaData()\n",
        "    le = BattleEcosystem(meta_data, debug=True)\n",
        "    n_epochs = 10\n",
        "\n",
        "    times1, wins_0_different,rate1, policy_name1 =different_teams(n_epochs,le,roster)\n",
        "    times2, wins_0_same, rate2, policy_name2 =same_team(n_epochs,le,roster)\n",
        "\n",
        "    print(f\"Player 0 con diverso team con {policy_name1} ha vinto: {wins_0_different} partite su {n_epochs}, win rate {rate1}, tempo impiegato {times1}\")\n",
        "    print(f\"Player 1 stesso team con {policy_name2} ha vinto: {wins_0_same} partite su {n_epochs}, win rate {rate2}, tempo impiegato {times2}\")\n",
        "\n",
        "\n",
        "\n",
        "def different_teams(n_epochs,le:BattleEcosystem,roster):\n",
        "\n",
        "    wins_player0 = 0\n",
        "    wins_player1 = 0\n",
        "    start_time = t.time()\n",
        "    for i in range(n_epochs):\n",
        "        cm1 = CompetitorManager(MyCompetitor2(\"Player 2\"))\n",
        "        team = RandomTeamFromRoster(roster).get_team()\n",
        "        # cm1.team = RandomTeamFromRoster(roster).get_team()\n",
        "        cm1.team = team\n",
        "        le.register(cm1)\n",
        "        cm2 = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "        team2 = RandomTeamFromRoster(roster).get_team()\n",
        "        cm2.team = team2\n",
        "        # cm2.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm2)\n",
        "        # Esegui una singola epoca\n",
        "        le.run(1)\n",
        "        # Stampa le vittorie di Player 0 e Player 1\n",
        "        wins_player0 += le.win_counts[cm1]\n",
        "        wins_player1 += le.win_counts[cm2]\n",
        "\n",
        "        le.unregister(cm1)\n",
        "        le.unregister(cm2)\n",
        "    end_time = t.time()\n",
        "    time = end_time - start_time\n",
        "\n",
        "    return time,wins_player0,(wins_player0/n_epochs)*100,cm1.competitor.battle_policy.name\n",
        "\n",
        "\n",
        "def same_team(n_epochs, le:BattleEcosystem,roster):\n",
        "\n",
        "    wins_player0 = 0\n",
        "    wins_player1 = 0\n",
        "    start_time = t.time()\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        cm1 = CompetitorManager(MyCompetitor2(\"Player 2\"))\n",
        "        cm1.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm1)\n",
        "        cm2 = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "        cm2.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm2)\n",
        "        # Esegui una singola epoca\n",
        "        le.run(1)\n",
        "        # Conta le vittorie di Player 0 e Player 1\n",
        "        wins_player0 += le.win_counts[cm1]\n",
        "        wins_player1 += le.win_counts[cm2]\n",
        "        le.unregister(cm1)\n",
        "        le.unregister(cm2)\n",
        "    end_time = t.time()\n",
        "    time = end_time-start_time\n",
        "\n",
        "    return time,wins_player0,(wins_player0/n_epochs)*100, cm1.competitor.battle_policy.name"
      ],
      "metadata": {
        "id": "6MPOG1Nds0pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Piccolo commento sui risultati"
      ],
      "metadata": {
        "id": "G8NC_KMDskvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Battle**\n",
        "\n",
        "Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player vs Random Player. The first result is when the two player have always a different team and the second when they have always the same team"
      ],
      "metadata": {
        "id": "E71rP0Pis1gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_PLAYERS = 2\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    roster = RandomPkmRosterGenerator().gen_roster()\n",
        "    meta_data = StandardMetaData()\n",
        "    le = BattleEcosystem(meta_data, debug=True)\n",
        "    n_epochs = 10\n",
        "\n",
        "    times1, wins_0_different,rate1, policy_name1 =different_teams(n_epochs,le,roster)\n",
        "    times2, wins_0_same, rate2, policy_name2 =same_team(n_epochs,le,roster)\n",
        "\n",
        "    print(f\"Player 0 con diverso team con {policy_name1} ha vinto: {wins_0_different} partite su {n_epochs}, win rate {rate1}, tempo impiegato {times1}\")\n",
        "    print(f\"Player 1 stesso team con {policy_name2} ha vinto: {wins_0_same} partite su {n_epochs}, win rate {rate2}, tempo impiegato {times2}\")\n",
        "\n",
        "\n",
        "\n",
        "def different_teams(n_epochs,le:BattleEcosystem,roster):\n",
        "\n",
        "    wins_player0 = 0\n",
        "    wins_player1 = 0\n",
        "    start_time = t.time()\n",
        "    for i in range(n_epochs):\n",
        "        cm1 = CompetitorManager(MyCompetitor3(\"Player 3\"))\n",
        "        team = RandomTeamFromRoster(roster).get_team()\n",
        "        # cm1.team = RandomTeamFromRoster(roster).get_team()\n",
        "        cm1.team = team\n",
        "        le.register(cm1)\n",
        "        cm2 = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "        team2 = RandomTeamFromRoster(roster).get_team()\n",
        "        cm2.team = team2\n",
        "        # cm2.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm2)\n",
        "        # Esegui una singola epoca\n",
        "        le.run(1)\n",
        "        # Stampa le vittorie di Player 0 e Player 1\n",
        "        wins_player0 += le.win_counts[cm1]\n",
        "        wins_player1 += le.win_counts[cm2]\n",
        "\n",
        "        le.unregister(cm1)\n",
        "        le.unregister(cm2)\n",
        "    end_time = t.time()\n",
        "    time = end_time - start_time\n",
        "\n",
        "    return time,wins_player0,(wins_player0/n_epochs)*100,cm1.competitor.battle_policy.name\n",
        "\n",
        "\n",
        "def same_team(n_epochs, le:BattleEcosystem,roster):\n",
        "\n",
        "    wins_player0 = 0\n",
        "    wins_player1 = 0\n",
        "    start_time = t.time()\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        cm1 = CompetitorManager(MyCompetitor3(\"Player 3\"))\n",
        "        cm1.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm1)\n",
        "        cm2 = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "        cm2.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm2)\n",
        "        # Esegui una singola epoca\n",
        "        le.run(1)\n",
        "        # Conta le vittorie di Player 0 e Player 1\n",
        "        wins_player0 += le.win_counts[cm1]\n",
        "        wins_player1 += le.win_counts[cm2]\n",
        "        le.unregister(cm1)\n",
        "        le.unregister(cm2)\n",
        "    end_time = t.time()\n",
        "    time = end_time-start_time\n",
        "\n",
        "    return time,wins_player0,(wins_player0/n_epochs)*100, cm1.competitor.battle_policy.name"
      ],
      "metadata": {
        "id": "7Ib_CBagtE-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "piccolo commento sui risultati"
      ],
      "metadata": {
        "id": "wL5eVE_XsiQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third Battle**\n",
        "Custom Player vs Random Player. The first result is when the two player have always a different team and the second when they have always the same team"
      ],
      "metadata": {
        "id": "53DgqD1EtGNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_PLAYERS = 2\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    roster = RandomPkmRosterGenerator().gen_roster()\n",
        "    meta_data = StandardMetaData()\n",
        "    le = BattleEcosystem(meta_data, debug=True)\n",
        "    n_epochs = 10\n",
        "\n",
        "    times1, wins_0_different,rate1, policy_name1 =different_teams(n_epochs,le,roster)\n",
        "    times2, wins_0_same, rate2, policy_name2 =same_team(n_epochs,le,roster)\n",
        "\n",
        "    print(f\"Player 0 con diverso team con {policy_name1} ha vinto: {wins_0_different} partite su {n_epochs}, win rate {rate1}, tempo impiegato {times1}\")\n",
        "    print(f\"Player 1 stesso team con {policy_name2} ha vinto: {wins_0_same} partite su {n_epochs}, win rate {rate2}, tempo impiegato {times2}\")\n",
        "\n",
        "\n",
        "\n",
        "def different_teams(n_epochs,le:BattleEcosystem,roster):\n",
        "\n",
        "    wins_player0 = 0\n",
        "    wins_player1 = 0\n",
        "    start_time = t.time()\n",
        "    for i in range(n_epochs):\n",
        "        cm1 = CompetitorManager(MyCompetitor0(\"Player 0\"))\n",
        "        team = RandomTeamFromRoster(roster).get_team()\n",
        "        # cm1.team = RandomTeamFromRoster(roster).get_team()\n",
        "        cm1.team = team\n",
        "        le.register(cm1)\n",
        "        cm2 = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "        team2 = RandomTeamFromRoster(roster).get_team()\n",
        "        cm2.team = team2\n",
        "        # cm2.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm2)\n",
        "        # Esegui una singola epoca\n",
        "        le.run(1)\n",
        "        # Stampa le vittorie di Player 0 e Player 1\n",
        "        wins_player0 += le.win_counts[cm1]\n",
        "        wins_player1 += le.win_counts[cm2]\n",
        "\n",
        "        le.unregister(cm1)\n",
        "        le.unregister(cm2)\n",
        "    end_time = t.time()\n",
        "    time = end_time - start_time\n",
        "\n",
        "    return time,wins_player0,(wins_player0/n_epochs)*100,cm1.competitor.battle_policy.name\n",
        "\n",
        "\n",
        "def same_team(n_epochs, le:BattleEcosystem,roster):\n",
        "\n",
        "    wins_player0 = 0\n",
        "    wins_player1 = 0\n",
        "    start_time = t.time()\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        cm1 = CompetitorManager(MyCompetitor0(\"Player 0\"))\n",
        "        cm1.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm1)\n",
        "        cm2 = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "        cm2.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm2)\n",
        "        # Esegui una singola epoca\n",
        "        le.run(1)\n",
        "        # Conta le vittorie di Player 0 e Player 1\n",
        "        wins_player0 += le.win_counts[cm1]\n",
        "        wins_player1 += le.win_counts[cm2]\n",
        "        le.unregister(cm1)\n",
        "        le.unregister(cm2)\n",
        "    end_time = t.time()\n",
        "    time = end_time-start_time\n",
        "\n",
        "    return time,wins_player0,(wins_player0/n_epochs)*100, cm1.competitor.battle_policy.name"
      ],
      "metadata": {
        "id": "n11v0WDgtR8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "piccolo commento sul risultato"
      ],
      "metadata": {
        "id": "Wpo7Urj7sen4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuornament**\n",
        "\n",
        "In this tournament all the player involved play agenst each other in this way. Every player plays against each other and the final result is reported in a table where the most winning player on top and the less winning player at the bottom. The numbers beside the name of the player represent the number of winnig battles. In every game the player played under the same conditions infact the team was always the same for every player and it was choosen randomly"
      ],
      "metadata": {
        "id": "B28q4sbFtPtd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNQTd5OntQje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Piccolo commento sul risultato"
      ],
      "metadata": {
        "id": "NIluu3wzsbgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "The result from the the simulations of the battle was allineated with what we thought. In fact every player with a policy different from the random one was able to defeat the random playeer, not always with outstanding results but they culd beat the random player. Our new evaluation function apllied to the minimax and to the other minimax with the alpha beta pruning and killer move heuristic function aimed to obtain a more aggressive player more focused on the attack fase and on the power of the moves in the roster. At the beginning we thougth that this could be the better approch and that battle would be ended in a very fast way. This was partialy true beacuse it was only partially more faster but the win rate was not as good as we fought maybe a more conservative approch wuolb be better. The best out of the 4 Player was the one with the custom policy. This because the custom policy takes into consideration variuos aspect of the game for example take into consideration when to do a switch between our pokemon and with whom to do this switch, so the policy was aggressive becauese aimed to search the best and powerful move but take into account also the matchup with the pokemon of the opponent, giving to the Custom policy a better knowledge of the state of the game (da rivedere)"
      ],
      "metadata": {
        "id": "szMZB4G-tZcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appendix**\n",
        "(se vogliamo metterci qualcosa ma vediamo dopo)"
      ],
      "metadata": {
        "id": "LbjeRHRbsR9t"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}