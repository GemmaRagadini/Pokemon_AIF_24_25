{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaRagadini/Pokemon_AIF_24_25/blob/main/pokemon-vgc-engine-master/example/Notebook_project_Pokemon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pokemon_Battle PokeBob team\n",
        "\n",
        "The project focuses on the track proposed in the course, specifically the section related to competitions. Our team selected the task involving the simulation of a Pokémon battle between two teams, each composed of three Pokémon. The battle consists of three matches, with the first player to knock out all three Pokémon of the opposing player declared the winner of the match. The battle is considered concluded when a player wins at least two out of three matches.\n",
        "\n",
        "The objective of the project was to develop a competitive AI agent in the Pokémon battle environment. We started by challenging a random player who selects its Pokémon moves arbitrarily without any strategic logic. Both players operate under the same conditions, with their teams assigned randomly and regenerated before each challenge, ensuring that any advantages or disadvantages are also determined by chance.\n",
        "In the following sections, we will explain our approach to solving the task and outline the methodology we adopted, along with the results we obtained.\n"
      ],
      "metadata": {
        "id": "uSQ9kvX2QiD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone https://github.com/GemmaRagadini/Pokemon_AIF_24_25.git\n",
        "%cd Pokemon_AIF_24_25/pokemon-vgc-engine-master\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BE_4RJnMlLfh",
        "outputId": "a7a1f71d-b14d-4c66-f987-7eb90dfc1b77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pokemon_AIF_24_25'...\n",
            "remote: Enumerating objects: 1940, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 1940 (delta 4), reused 0 (delta 0), pack-reused 1909 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1940/1940), 21.65 MiB | 25.99 MiB/s, done.\n",
            "Resolving deltas: 100% (481/481), done.\n",
            "/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/Pokemon_AIF_24_25/pokemon-vgc-engine-master\n",
            "CHANGELOG    example\t  organization\trequirements.txt  test\n",
            "competition  LICENSE.txt  README.md\tsetup.py\t  vgc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "x7AhyiJjZjDJ",
        "outputId": "23d83d9a-d4b0-4370-c79f-b0bdab434620"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arcade~=2.6.17 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.17)\n",
            "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: gymnasium~=0.29.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.29.1)\n",
            "Requirement already satisfied: customtkinter~=5.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (5.2.2)\n",
            "Requirement already satisfied: torch~=2.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.2.2)\n",
            "Requirement already satisfied: pygad~=3.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.3.1)\n",
            "Requirement already satisfied: scipy~=1.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.12.0)\n",
            "Requirement already satisfied: setuptools~=69.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (69.1.1)\n",
            "Requirement already satisfied: pyglet==2.0.dev23 in /usr/local/lib/python3.11/dist-packages (from arcade~=2.6.17->-r requirements.txt (line 1)) (2.0.dev23)\n",
            "Requirement already satisfied: pillow~=9.3.0 in /usr/local/lib/python3.11/dist-packages (from arcade~=2.6.17->-r requirements.txt (line 1)) (9.3.0)\n",
            "Requirement already satisfied: pymunk~=6.4.0 in /usr/local/lib/python3.11/dist-packages (from arcade~=2.6.17->-r requirements.txt (line 1)) (6.4.0)\n",
            "Requirement already satisfied: pytiled-parser==2.2.0 in /usr/local/lib/python3.11/dist-packages (from arcade~=2.6.17->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from pytiled-parser==2.2.0->arcade~=2.6.17->-r requirements.txt (line 1)) (24.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pytiled-parser==2.2.0->arcade~=2.6.17->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium~=0.29.1->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium~=0.29.1->-r requirements.txt (line 3)) (0.0.4)\n",
            "Requirement already satisfied: darkdetect in /usr/local/lib/python3.11/dist-packages (from customtkinter~=5.2.2->-r requirements.txt (line 4)) (0.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from customtkinter~=5.2.2->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.2.1->-r requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.2.1->-r requirements.txt (line 5)) (12.6.85)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pygad~=3.3.1->-r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: cffi>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from pymunk~=6.4.0->arcade~=2.6.17->-r requirements.txt (line 1)) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.2.1->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygad~=3.3.1->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygad~=3.3.1->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygad~=3.3.1->-r requirements.txt (line 6)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygad~=3.3.1->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygad~=3.3.1->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygad~=3.3.1->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch~=2.2.1->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.15.0->pymunk~=6.4.0->arcade~=2.6.17->-r requirements.txt (line 1)) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pygad~=3.3.1->-r requirements.txt (line 6)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv amb\n",
        "!source amb/bin/activate"
      ],
      "metadata": {
        "id": "1DHAISC0tzu5",
        "outputId": "35126774-820e-462b-d8a8-3bd21fc6d620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.11/dist-packages (20.29.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (0.3.9)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.6)\n",
            "created virtual environment CPython3.11.11.final.0-64 in 1224ms\n",
            "  creator CPython3Posix(dest=/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/Pokemon_AIF_24_25/pokemon-vgc-engine-master/amb, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==24.3.1, setuptools==75.8.0, wheel==0.45.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source amb/bin/activate"
      ],
      "metadata": {
        "id": "3snvUrVWu5uK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Related Work\n",
        "\n",
        "For this project, we decided to explore some existing approaches related to the concepts studied during the course, as well as to develop a custom approach of our own. These approaches were evaluated through a tournament, the details of which will be provided in the corresponding section.\n",
        "\n",
        "The related work was derived from the course slides and the accompanying textbook, *Artificial Intelligence A Modern Approach - Fourth Edition. Stuart Russer, Peter Norvig*. Specifically, we focused on the section related to game theory (Chapter 6 of the textbook) and examined various approaches to the Minimax algorithm and its variations. This included the implementation of alpha-beta pruning and heuristics such as the killer move heuristic."
      ],
      "metadata": {
        "id": "czJQB6UsZVMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "To achieve our goal, we decided to implement several algorithms discussed in the related work. We focused our attention on various implementations of the Minimax algorithm using 2 different evaluation functions. In the following sections, we will provide a detailed explanation of each algorithm we implemented."
      ],
      "metadata": {
        "id": "RLuFhPYL_ZQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List\n",
        "from vgc.behaviour import evalFunctions\n",
        "from vgc.datatypes.Types import PkmStat, PkmType, WeatherCondition\n",
        "from vgc.datatypes.Objects import Pkm, GameState,PkmType, PkmTeam, PkmStat\n",
        "from vgc.datatypes.Constants import DEFAULT_PKM_N_MOVES, DEFAULT_PARTY_SIZE, TYPE_CHART_MULTIPLIER, DEFAULT_N_ACTIONS\n",
        "from vgc.behaviour import BattlePolicy\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "uiuBcrxL7hDO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implemented Policies\n",
        "\n",
        "We implemented 3 algorithms: classic Minimax, Minimax with alpha-beta pruning and killer heuristic, and My Policy. The third one is the policy that performed the best. The first two were tested with two different evaluation functions, `game_state_eval` and `my_eval_fun`.\n",
        "\n",
        "The two Minimax algorithms are located in the file `behaviour/otherPolicies.py`, while `MyPolicy` is located in the file `behaviour/myPolicy.py`.\n",
        "\n",
        "\n",
        "# Minimax\n",
        "\n",
        "The first implementation is the one obout a classic minimax algorithm with a pre-existent evaluation function called `game_eval`. Here below there is our implementation.\n"
      ],
      "metadata": {
        "id": "71GjO_fpBQUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMinimax(BattlePolicy):\n",
        "\n",
        "    def __init__(self, max_depth: int = 4):\n",
        "        self.max_depth = max_depth\n",
        "        self.name = \"My Minimax\"\n",
        "\n",
        "    def minimax(self, g, depth, is_maximizing_player):\n",
        "        \"\"\"\n",
        "        Classic minimax\n",
        "        \"\"\"\n",
        "\n",
        "        if depth == 0:\n",
        "            # Evaluate the current state\n",
        "            try:\n",
        "                evaluation = evalFunctions.game_state_eval(g,depth)\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            return evaluation , None\n",
        "\n",
        "        if is_maximizing_player: # MAX\n",
        "            max_eval = float('-inf')\n",
        "            best_action = None\n",
        "            try:\n",
        "                for i in range(DEFAULT_N_ACTIONS):\n",
        "                    g_copy = deepcopy(g)\n",
        "                    s, _, _, _, _ = g_copy.step([i, 99])  # The opponent performs an invalid action that does not change the situation\n",
        "                    if evalFunctions.n_defeated(s[0].teams[0]) > evalFunctions.n_defeated(g.teams[0]):\n",
        "                        continue # Ignore states where our number of defeated Pokémon increases\n",
        "                    eval_score, _ = self.minimax(s[0], depth - 1, False)\n",
        "                    if eval_score > max_eval:\n",
        "                        max_eval = eval_score\n",
        "                        best_action = i\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            return max_eval, best_action\n",
        "\n",
        "        else:  # MIN\n",
        "            min_eval = float('inf')\n",
        "            best_action = None\n",
        "            try:\n",
        "                for j in range(DEFAULT_N_ACTIONS):\n",
        "                    g_copy = deepcopy(g)\n",
        "                    s, _, _, _, _ = g_copy.step([99, j])\n",
        "                    if evalFunctions.n_defeated(s[0].teams[1]) > evalFunctions.n_defeated(g.teams[1]):\n",
        "                        continue\n",
        "                    eval_score, _ = self.minimax(s[0], depth - 1, True)\n",
        "                    if eval_score < min_eval:\n",
        "                        min_eval = eval_score\n",
        "                        best_action = j\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            return min_eval, best_action\n",
        "\n",
        "\n",
        "    def get_action(self, g) -> int:\n",
        "        \"\"\"\n",
        "        Find the best action for MAX\n",
        "        \"\"\"\n",
        "        try:\n",
        "            _, best_action = self.minimax(g, self.max_depth, True)\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        return best_action if best_action is not None else 0\n"
      ],
      "metadata": {
        "id": "WtNN0DTFC2Bv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell above, we present our first implementation of the Minimax policy, which, in this case, employs `game_state_eval` evaluation functions. This function is described in the *Evaluation Functions* section.\n",
        "\n",
        "The Minimax implementation is straightforward, comprising a section for the maximizer player and another for the minimizer. The maximizer aims to transition to states where its Pokémon are healthier than the opponent’s Pokémon, while the minimizer seeks to reduce this advantage. Each state is evaluated recursively.\n",
        "\n",
        "To support these computations, the algorithm uses the `n_defeated` function (in `behaviour/evalFunctions.py`, again in *Evaluation Functions* section), which counts the number of fainted (knocked-out) Pokémon. Additionally, the algorithm determines the next action from the maximizer player’s perspective, as implemented in the `get_action` method.\n",
        "\n",
        "The default values for the search depth and weights in the evaluation function were determined empirically. Various configurations were tested, and the ones used here were found to deliver the best performance according to our evaluation metrics.\n"
      ],
      "metadata": {
        "id": "TX6rZ_AqEoUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimax with Alpha-Beta Pruning and Killer Move Heuristic\n",
        "\n",
        "Our second implementation extends the basic Minimax algorithm by incorporating alpha-beta pruning and the killer move heuristic. This implementation was developed to enhance both the performance and efficiency of the Minimax algorithm described above.\n",
        "\n",
        "The addition of alpha-beta pruning allows the algorithm to eliminate branches in the search tree that cannot influence the final decision, significantly reducing the number of nodes explored. Meanwhile, the killer move heuristic prioritizes moves that are likely to be effective, further optimizing the decision-making process by focusing on promising actions.\n",
        "\n",
        "The combined use of these techniques aims to not only improve the accuracy of the algorithm but also speed up its execution, enabling faster and more effective decision-making.\n",
        "\n",
        "In the cells below, we present our implementation of this enhanced Minimax algorithm along with a detailed explanation of how it operates."
      ],
      "metadata": {
        "id": "mk6j3adLRxU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMinimaxWithAlphaBetaKiller(BattlePolicy):\n",
        "    \"\"\"\n",
        "    Minimax algorithm with alpha beta pruning and killer moves optimization\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth: int = 5):\n",
        "        self.max_depth = max_depth\n",
        "        self.name = \"Minimax with pruning alpha beta killer\"\n",
        "        self.killer_moves = {depth: [] for depth in range(max_depth + 1)}\n",
        "\n",
        "    def minimax(self, g, depth, alpha, beta, is_maximizing_player):\n",
        "        if depth == 0:\n",
        "            try:\n",
        "                evaluation = evalFunctions.game_state_eval(g,depth)\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            return evaluation, None\n",
        "\n",
        "        if is_maximizing_player:\n",
        "            max_eval = float('-inf')\n",
        "            best_action = None\n",
        "\n",
        "            # Possible actions\n",
        "            moves = list(range(DEFAULT_N_ACTIONS))\n",
        "            # prioritizing killer moves\n",
        "            killer_moves = self.killer_moves.get(depth, [])\n",
        "            moves = sorted(moves, key=lambda move: move in killer_moves, reverse=True)\n",
        "\n",
        "            try:\n",
        "                for i in moves:\n",
        "                    g_copy = deepcopy(g)\n",
        "                    s, _, _, _, _ = g_copy.step([i, 99])\n",
        "                    if evalFunctions.n_defeated(s[0].teams[0]) > evalFunctions.n_defeated(g.teams[0]):\n",
        "                        continue\n",
        "\n",
        "                    eval_score, _ = self.minimax(s[0], depth - 1, alpha, beta, False)\n",
        "                    if eval_score > max_eval:\n",
        "                        max_eval = eval_score\n",
        "                        best_action = i\n",
        "\n",
        "                    alpha = max(alpha, eval_score)\n",
        "                    if beta <= alpha:\n",
        "                        # update killer moves\n",
        "                        if i not in self.killer_moves[depth]:\n",
        "                            self.killer_moves[depth].append(i)\n",
        "                            if len(self.killer_moves[depth]) > 2:\n",
        "                                self.killer_moves[depth].pop(0)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            return max_eval, best_action\n",
        "\n",
        "        else:\n",
        "            min_eval = float('inf')\n",
        "            best_action = None\n",
        "\n",
        "            moves = list(range(DEFAULT_N_ACTIONS))\n",
        "\n",
        "            try:\n",
        "                killer_moves = self.killer_moves.get(depth, [])\n",
        "                moves = sorted(moves, key=lambda move: move in killer_moves, reverse=True)\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            try:\n",
        "                for j in moves:\n",
        "                    g_copy = deepcopy(g)\n",
        "                    s, _, _, _, _ = g_copy.step([99, j])\n",
        "                    if evalFunctions.n_defeated(s[0].teams[1]) > evalFunctions.n_defeated(g.teams[1]):\n",
        "                        continue\n",
        "\n",
        "                    eval_score, _ = self.minimax(s[0], depth - 1, alpha, beta, True)\n",
        "                    if eval_score < min_eval:\n",
        "                        min_eval = eval_score\n",
        "                        best_action = j\n",
        "\n",
        "                    beta = min(beta, eval_score)\n",
        "                    if beta <= alpha:\n",
        "                        if j not in self.killer_moves[depth]:\n",
        "                            self.killer_moves[depth].append(j)\n",
        "                            if len(self.killer_moves[depth]) > 2:\n",
        "                                self.killer_moves[depth].pop(0)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "            return min_eval, best_action\n",
        "\n",
        "    def get_action(self, g) -> int:\n",
        "        try:\n",
        "            _, best_action = self.minimax(g, self.max_depth, float('-inf'), float('inf'), True)\n",
        "        except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "        return best_action if best_action is not None else 0\n"
      ],
      "metadata": {
        "id": "nRhf_-spRwrb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we illustrate the key components of this algorithm.\n",
        "\n",
        "Alpha-Beta Pruning is a fundamental optimization technique used in Minimax algorithms as we said before. It reduces the number of nodes evaluated in the game tree by eliminating branches that cannot influence the final decision. Specifically:\n",
        "Alpha represents the best score achievable by the maximizing player, ensuring the current node's value is not less than this threshold.\n",
        "Beta represents the best score achievable by the minimizing player, ensuring the current node's value does not exceed this threshold. By comparing node evaluations against these bounds, the algorithm can skip unnecessary evaluations and focus on the most promising branches.\n",
        "The Killer Move Heuristics prioritize actions (or moves) that previously led to a cutoff during Alpha-Beta Pruning at the same depth. These \"killer moves\" are stored in a depth-specific list and revisited with high priority in subsequent iterations. The rationale is that actions causing significant pruning in similar scenarios are likely to be effective again, reducing the time spent on less impactful moves.\n",
        "\n",
        "The algorithm explores the game tree up to a predefined depth, *max_depth*. At this point, it evaluates the game state using a heuristic evaluation function,the same of the latest implementation of Minimax. This depth limit balances the trade-off between computational feasibility and decision quality and it was selected in a empirical way.\n",
        "\n",
        "**Algorithm Workflow**\n",
        "\n",
        "The algorithm begins by initializing necessary parameters, including the maximum search depth and a set of *killer moves* for each depth level. These killer moves are essentially a list of actions that have proven effective in causing cutoffs during previous searches. By storing these moves, the algorithm can prioritize their exploration in subsequent iterations, aiming to reduce unnecessary computations.\n",
        "\n",
        "Once initialized, the algorithm proceeds to explore the game tree using the Minimax method. This exploration is depth-limited, meaning the algorithm only evaluates the game tree to a specific depth (*max_depth*) to keep the computation manageable. At the root node, the algorithm decides whether it is currently the turn of the maximizing or minimizing player and selects actions accordingly.\n",
        "\n",
        "For the maximizing player, the goal is to find the action that yields the highest evaluation score and where the number of fainted pokemon is higher for the opponent, representing the most advantageous outcome. Conversely, for the minimizing player, the focus is on selecting the action that minimizes the evaluation score, simulating an opponent trying to counteract the maximizing player’s strategies. At each level of the game tree, Alpha and Beta values are used to dynamically track the best and worst outcomes possible for the respective players. These values guide the pruning process, helping the algorithm decide when to stop exploring certain branches.\n",
        "\n",
        "The algorithm introduces a prioritization mechanism at this stage by sorting available actions based on the killer moves. If a move caused a cutoff at the same depth in a previous search, it is considered likely to do so again and is explored first. This ensures the algorithm focuses on the most promising actions early on, increasing the chances of quickly achieving cutoffs. For example, if the algorithm identifies a move that significantly improves the maximizing player’s position, it will immediately consider pruning all remaining moves that cannot surpass this outcome.\n",
        "\n",
        "As the game tree is traversed, the algorithm continuously updates the Alpha and Beta values. When a node’s evaluation score falls outside the range defined by Alpha and Beta, the algorithm terminates further exploration of that branch. This process, known as pruning, saves computational resources by avoiding the evaluation of irrelevant or unpromising paths. If a cutoff occurs, the current move is added to the list of killer moves for the corresponding depth, ensuring that it will be prioritized in future iterations.\n",
        "\n",
        "Once all possible actions are evaluated, the algorithm determines the best action for the maximizing player at the root node."
      ],
      "metadata": {
        "id": "i8hRaI0RkecE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Functions\n",
        "\n",
        "The evaluation function plays a central role in determining the quality of a game state during a Pokémon battle simulation.\n",
        "\n",
        "The purpose of the evaluation function is to return a numerical score that represents the desirability of the current game state. Higher scores indicate more favorable conditions for the agent, while lower scores highlight disadvantages.\n",
        "\n",
        "`game_state_eval` is the pre-existent evaluation function, it encourages states where the player’s active Pokémon (mine) has higher HP relative to its maximum HP and penalizes states where the opponent’s active Pokémon (opp) has high HP.\n",
        "Finally it adds a penalty proportional to the search depth to prioritize faster victories.\n",
        "Our results demonstrate that it provides a balanced implementation. However, it is not the most effective approach we encountered.\n",
        "\n",
        "To try to improve the performance of the algorithms, we implemented another evaluation function, `my_eval_fun` that builds upon the previous one.\n",
        "The function achieves this by evaluating three core aspects:\n",
        "\n",
        "*Type Compatibility*: The function evaluates the matchup between the agent's active Pokémon and the opponent's active Pokémon. This includes assessing whether the agent's Pokémon has moves that are particularly effective against the opponent. For example, if the agent's Pokémon has a \"super effective\" move available, the function assigns a higher score to reflect this strategic advantage.\n",
        "\n",
        "*Health Points (HP) Analysis*: we used the base evaluation function, `game_state_eval`, to measure the state of the Pokémon's HP as we did in the previous minimax implementation. This component assesses the agent’s ability to maintain survivability by factoring in both the agent's and the opponent's remaining HP.\n",
        "\n",
        "*Damage Potential*: The function estimates the maximum damage the agent's Pokémon can inflict on the opponent in the current turn. This is done by considering the Pokémon's strongest available move and adjusting for various factors such as type effectiveness, stat boosts or reductions, and environmental conditions like weather. The damage is then normalized to ensure it aligns proportionally with the other components of the evaluation.\n",
        "\n",
        "These values are combined into a single score using weighted contributions:\n",
        "the damage potential and matchup analysis are scaled to ensure they have comparable influence on the final score.\n",
        "Health points are adjusted with an offset to reflect their importance without overpowering the other factors.\n",
        "Together, these components create a balanced evaluation of the game state.\n"
      ],
      "metadata": {
        "id": "0jJYj_LsyVqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def game_state_eval(s: GameState, depth):\n",
        "    \"\"\"\n",
        "    Pre-existing evaluation function\n",
        "    \"\"\"\n",
        "    mine = s.teams[0].active\n",
        "    opp = s.teams[1].active\n",
        "    return mine.hp / mine.max_hp - 3 * opp.hp / opp.max_hp - 0.3 * depth\n",
        "\n",
        "def estimate_damage(move_type: PkmType, pkm_type: PkmType, move_power: float, opp_pkm_type: PkmType,\n",
        "                    attack_stage: int, defense_stage: int, weather: WeatherCondition) -> float:\n",
        "    \"\"\"\n",
        "    (pre-existing)\n",
        "    Estimate the damage that the move moveType deals to the opposing Pokémon, taking into account:\n",
        "    - types of both Pokémon\n",
        "    - type of the move\n",
        "    - attack and defense stats\n",
        "    - weather\n",
        "    - move power\n",
        "    \"\"\"\n",
        "    # Bonus for same-type move used by the Pokémon\n",
        "    stab = 1.5 if move_type == pkm_type else 1.\n",
        "    # Favorable weather\n",
        "    if (move_type == PkmType.WATER and weather == WeatherCondition.RAIN) or (\n",
        "            move_type == PkmType.FIRE and weather == WeatherCondition.SUNNY):\n",
        "        weather = 1.5\n",
        "    # Unfavorable weather\n",
        "    elif (move_type == PkmType.WATER and weather == WeatherCondition.SUNNY) or (\n",
        "            move_type == PkmType.FIRE and weather == WeatherCondition.RAIN):\n",
        "        weather = .5\n",
        "    else:\n",
        "        weather = 1.\n",
        "    # relative level attack - defense\n",
        "    stage_level = attack_stage - defense_stage # in [-10,10]\n",
        "    # Multiplier that increases linearly for positive values and applies fractional reductions for negative values\n",
        "    stage = (stage_level + 2.) / 2 if stage_level >= 0. else 2. / (np.abs(stage_level) + 2.) # in [0,6]\n",
        "    # damage estimation\n",
        "    damage = TYPE_CHART_MULTIPLIER[move_type][opp_pkm_type] * stab * weather * stage * move_power # Approximately 140\n",
        "    return damage\n",
        "\n",
        "def my_eval_fun(s:GameState, depth):\n",
        "    \"\"\"\n",
        "    Our evaluation function that considers the compatibility between Pokémon,\n",
        "    the game_state_eval with respect to hp, and the ability to deal damage\n",
        "    \"\"\"\n",
        "    my_active = s.teams[0].active\n",
        "    opp_active = s.teams[1].active\n",
        "    attack_stage = s.teams[0].stage[PkmStat.ATTACK]\n",
        "    defense_stage = s.teams[1].stage[PkmStat.DEFENSE]\n",
        "    matchup = examine_matchup(my_active.type, opp_active.type, list(map(lambda m: m.type, my_active.moves))) # in [0,2]\n",
        "    eval_hp = game_state_eval(s,depth) + 4 # in [0-5]\n",
        "    max_damage = maxDamage(my_active, opp_active.type, attack_stage, defense_stage, s.weather) # in [0,140]\n",
        "    return max_damage/70 + matchup/2 + eval_hp\n",
        "\n",
        "\n",
        "def maxDamage(my_active: Pkm, opp_active_type:PkmType, attack_stage: int, defense_stage: int,weather: WeatherCondition ):\n",
        "    \"\"\"\n",
        "    Returns the maximum damage the active Pokémon can deal to the opponent with a move\n",
        "    \"\"\"\n",
        "    mvs_damage = []\n",
        "    # estimate the damage for each move of my Pokémon\n",
        "    for m in my_active.moves:\n",
        "        mvs_damage.append(estimate_damage(m.type,my_active.type, m.power, opp_active_type ,attack_stage, defense_stage, weather))\n",
        "    return np.max(mvs_damage)\n",
        "\n",
        "\n",
        "def examine_matchup(pkm_type: PkmType, opp_pkm_type: PkmType, moves_type: List[PkmType]) -> float:\n",
        "    \"\"\"\n",
        "    Evaluates the matchup between the active Pokémon and the opponent's Pokémon,\n",
        "    considering the types of the Pokémon and the available moves.\n",
        "    \"\"\"\n",
        "    for mtype in moves_type: # search for super effective move\n",
        "        if TYPE_CHART_MULTIPLIER[mtype][pkm_type] == 2.0:\n",
        "            return 2.0  # if there is a super effective move\n",
        "    # Consider only the evaluation based on the Pokémon's type\n",
        "    return TYPE_CHART_MULTIPLIER[opp_pkm_type][pkm_type]\n",
        "\n",
        "def n_defeated(t: PkmTeam):\n",
        "    \"\"\"\n",
        "    compute the number of defeated pokemon in the team\n",
        "    \"\"\"\n",
        "    fainted = 0\n",
        "    fainted += t.active.hp == 0\n",
        "    if len(t.party) > 0:\n",
        "        fainted += t.party[0].hp == 0\n",
        "    if len(t.party) > 1:\n",
        "        fainted += t.party[1].hp == 0\n",
        "    return fainted"
      ],
      "metadata": {
        "id": "gsw3H7stDlhM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Policy\n",
        "\n",
        "The third and last approch was the one about a our custom Policy, implemented in `behaviour/myPolicy.py`."
      ],
      "metadata": {
        "id": "dYhf6x63nQWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPolicy(BattlePolicy):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.hail_used = False\n",
        "        self.sandstorm_used = False\n",
        "        self.name = \"My Policy\"\n",
        "\n",
        "    def assess_damages(self, active_pkm: Pkm, opp_pkm_type: PkmType, attack_stage: int, defense_stage: int, weather: WeatherCondition)-> int:\n",
        "        # moves evaluation\n",
        "        damages: List[float] = []\n",
        "        for move in active_pkm.moves:\n",
        "            damages.append(evalFunctions.estimate_damage(move.type, active_pkm.type, move.power, opp_pkm_type, attack_stage,\n",
        "                                          defense_stage, weather))\n",
        "        return damages\n",
        "\n",
        "\n",
        "    def get_action(self, g: GameState) -> int:\n",
        "        # my team\n",
        "        my_team = g.teams[0]\n",
        "        active_pkm = my_team.active\n",
        "        bench = my_team.party\n",
        "        my_attack_stage = my_team.stage[PkmStat.ATTACK]\n",
        "\n",
        "        # opposite team\n",
        "        opp_team = g.teams[1]\n",
        "        opp_active_pkm = opp_team.active\n",
        "        opp_defense_stage = opp_team.stage[PkmStat.DEFENSE]\n",
        "\n",
        "        # weather\n",
        "        weather = g.weather.condition\n",
        "\n",
        "        try:\n",
        "            # estimate of the damage of each move\n",
        "            damages = self.assess_damages(active_pkm, opp_active_pkm.type, my_attack_stage, opp_defense_stage, weather)\n",
        "            move_id = int(np.argmax(damages))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # if it eliminates the opponent or the move type is super effective, use it immediately\n",
        "        if (damages[move_id] >= opp_active_pkm.hp) or (damages[move_id] > 0 and TYPE_CHART_MULTIPLIER[active_pkm.moves[move_id].type][opp_active_pkm.type] == 2.0) :\n",
        "            return move_id\n",
        "        try:\n",
        "            defense_type_multiplier = evalFunctions.examine_matchup(active_pkm.type, opp_active_pkm.type,\n",
        "                                                    list(map(lambda m: m.type, opp_active_pkm.moves)))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        if defense_type_multiplier <= 1.0:\n",
        "            return move_id\n",
        "\n",
        "        # Consider the Pokémon switch\n",
        "        matchup: List[float] = []\n",
        "        not_fainted = False\n",
        "\n",
        "        try:\n",
        "            for j in range(len(bench)):\n",
        "                if bench[j].hp == 0.0:\n",
        "                    matchup.append(0.0)\n",
        "                else:\n",
        "                    not_fainted = True\n",
        "                    matchup.append(\n",
        "                        evalFunctions.examine_matchup(bench[j].type, opp_active_pkm.type, list(map(lambda m: m.type, bench[j].moves))))\n",
        "\n",
        "            best_switch_matchup = int(np.max(matchup))\n",
        "            best_switch = np.argmax(matchup)\n",
        "            current_matchup = evalFunctions.examine_matchup(active_pkm.type, opp_active_pkm.type,list(map(lambda m: m.type, active_pkm.moves)))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        if not_fainted and best_switch_matchup >= current_matchup+1:\n",
        "            return best_switch + 4\n",
        "\n",
        "        return move_id"
      ],
      "metadata": {
        "id": "mO64wj0PnP9M"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The policy is designed to make decisions about the actions the player's team should take based on an evaluation of damage, the Pokémon’s types, and various other battle conditions, such as weather. Here's an overview of its functionality:\n",
        "\n",
        "*Initialization*: The class initializes two flags, hail_used and sandstorm_used, to track if certain weather conditions have already been activated during the battle.\n",
        "\n",
        "*Damage Assessment* (`assess_damages`):\n",
        "This method evaluates the potential damage of each move available to the active Pokémon, using `evalFunctions.estimate_damage`. It calculates the damage based on various factors such as:\n",
        "\n",
        "  - The Pokémon's attack and the opponent's defense stages.\n",
        "  - The Pokémon types and move types.\n",
        "  - Weather conditions that might affect damage output.\n",
        "\n",
        "It then returns a list of damage estimates for all available moves.\n",
        "\n",
        "*Action Selection* (`get_action`):\n",
        "This is the primary method used to decide the next action. It follows a series of steps to make the decision:\n",
        "\n",
        "  - Team Setup: It first extracts the active Pokémon from the player's team and the opponent's team.\n",
        "  - Weather Condition: It retrieves the current weather condition, which can influence the effectiveness of moves.\n",
        "  - Damage Calculation: Using the `assess_damages` method, it calculates the potential damage for each move and selects the move with the highest estimated damage.\n",
        "\n",
        "*Move Selection Logic*: If a move will eliminate the opponent or if it is \"super effective\" (based on the type chart), it is selected immediately. If the active Pokémon is at a disadvantage in terms of move effectiveness, the policy evaluates to switch to a Pokémon from the bench (the reserve Pokémon) that has a better matchup against the opponent's active Pokémon. This is determined by evaluating the compatibility between each Pokémon on the bench and the opponent’s active Pokémon. The policy uses a comparison between the matchups of the active Pokémon and each bench Pokémon, selecting the Pokémon that has a significantly better type advantage.\n",
        "\n",
        "*Pokémon Switch Consideration*: If there is at least one Pokémon on the bench that has a favorable matchup compared to the active Pokémon, the policy will switch to that Pokémon. This is done by comparing the matchup values, and if the bench Pokémon’s matchup score is sufficiently better (greater than the current active Pokémon's matchup by at least 1), it will choose to switch to that Pokémon. When switching Pokémon, \"time\" is lost in battle, so the policy chooses to do so only if the gain is considerable.\n",
        "\n",
        "The policy uses the `evaluate_matchup` function to assess the compatibility between Pokémon types based on their moves, making it a dynamic policy that adapts to the strengths and weaknesses of the opposing team.\n",
        "\n",
        "This battle policy uses a combination of damage estimation, type advantages, and Pokémon switching strategies to make optimal decisions, ensuring that the player can both maximize damage and strategically manage their team for better performance in battle."
      ],
      "metadata": {
        "id": "Z1VhZfhmqNn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Evaluation\n",
        "\n",
        "We implemented several approaches and first thing we decided to have each agent battle against the Random agent. This allowed us to demonstrate that each policy outperforms the Random agent.\n",
        "\n",
        "We chose to use a random team for each player, generated for every match, so that with a large number of executions, the influence of the chosen team on the evaluation of the algorithm's performance decreases.\n",
        "\n",
        "For the second evaluation, we organized a tournament involving these players:\n",
        "\n",
        "- *My Policy Player*\n",
        "- *Random Player*\n",
        "- *Minimax Player*\n",
        "- *Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player*\n",
        "- *Minimax Player using `my_eval_fun`*\n",
        "- *Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player using `my_eval_fun`*\n",
        "\n"
      ],
      "metadata": {
        "id": "M43Al93JqRyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time as t\n",
        "from example.Example_Competitor import MyCompetitor0, MyCompetitor1, MyCompetitor2, MyCompetitor3, MyCompetitor4, MyCompetitor5\n",
        "from vgc.balance.meta import StandardMetaData\n",
        "from vgc.competition.Competitor import CompetitorManager\n",
        "from vgc.ecosystem.BattleEcosystem import BattleEcosystem\n",
        "from vgc.util.generator.PkmRosterGenerators import RandomPkmRosterGenerator\n",
        "from vgc.util.generator.PkmTeamGenerators import RandomTeamFromRoster\n",
        "import sys\n",
        "import random\n",
        "\n",
        "N_PLAYERS = 2\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    roster = RandomPkmRosterGenerator().gen_roster()\n",
        "    meta_data = StandardMetaData()\n",
        "    le = BattleEcosystem(meta_data, debug=True)\n",
        "    n_epochs = 10\n",
        "\n",
        "    times1, wins_0_different,rate1, policy_name1 = SingleCombat(n_epochs,le,roster)\n",
        "\n",
        "    print(f\"Player 0 con {policy_name1} ha vinto: {wins_0_different} partite su {n_epochs}, win rate {rate1}, tempo impiegato {times1}\")\n",
        "\n",
        "\n",
        "def SingleCombat(n_epochs, le:BattleEcosystem,roster):\n",
        "\n",
        "    wins_player0 = 0\n",
        "    wins_player1 = 0\n",
        "    start_time = t.time()\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        print(f\"Epoch {i} of {n_epochs}\")\n",
        "        cm1 = CompetitorManager(MyCompetitor0(\"Player 0\"))\n",
        "        cm1.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm1)\n",
        "        cm2 = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "        cm2.team = RandomTeamFromRoster(roster).get_team()\n",
        "        le.register(cm2)\n",
        "        # single epoch\n",
        "        le.run(1)\n",
        "        # counts wins of Player 0 and Player 1\n",
        "        wins_player0 += le.win_counts[cm1]\n",
        "        wins_player1 += le.win_counts[cm2]\n",
        "        le.unregister(cm1)\n",
        "        le.unregister(cm2)\n",
        "    end_time = t.time()\n",
        "    time = end_time-start_time\n",
        "\n",
        "    return time,wins_player0,(wins_player0/n_epochs)*100, cm1.competitor.battle_policy.name\n",
        "\n",
        "\n",
        "\n",
        "def Tournament():\n",
        "\n",
        "    print(\"Let the Tournament begin\")\n",
        "\n",
        "    roster = RandomPkmRosterGenerator().gen_roster()\n",
        "    meta_data = StandardMetaData()\n",
        "\n",
        "    # trainers\n",
        "    Pokebob = CompetitorManager(MyCompetitor0(\"Player 0\"))\n",
        "    randomtrainer = CompetitorManager(MyCompetitor1(\"Player 1\"))\n",
        "    minimax = CompetitorManager(MyCompetitor2(\"Player 2\"))\n",
        "    minimax_killer= CompetitorManager(MyCompetitor3(\"Player 3\"))\n",
        "    minimax_my_eval = CompetitorManager(MyCompetitor4(\"Player 4\"))\n",
        "    minimax_killer_my_eval = CompetitorManager(MyCompetitor5(\"Player 5\"))\n",
        "    trainers= [Pokebob, randomtrainer, minimax,minimax_killer, minimax_my_eval, minimax_killer_my_eval]\n",
        "\n",
        "    scores = [0] * len(trainers)\n",
        "\n",
        "    n_epochs = 10 # number epochs for each match\n",
        "\n",
        "    # italian tournament with each trainer having a randomly generated team for each battle\n",
        "    print(\"Tournament\")\n",
        "    for i in range(len(trainers)):\n",
        "        for j in range(i + 1, len(trainers)):  # Avoid duplicate matches\n",
        "            #print(f\"Begin Match between {trainers[i].competitor.battle_policy.name} and {trainers[j].competitor.battle_policy.name}\")\n",
        "            wins_player0 = 0\n",
        "            wins_player1 = 0\n",
        "            for k in range(n_epochs):\n",
        "                #print(f\"Epoch {k} of {n_epochs}\")\n",
        "                try:\n",
        "                    le = BattleEcosystem(meta_data, debug=True)\n",
        "                    trainers[i].team = RandomTeamFromRoster(roster).get_team()\n",
        "                    trainers[j].team = RandomTeamFromRoster(roster).get_team()\n",
        "                    le.register(trainers[i])\n",
        "                    le.register(trainers[j])\n",
        "                    # match\n",
        "                    le.run(1)\n",
        "                    wins_player0 += le.win_counts[trainers[i]]\n",
        "                    wins_player1 += le.win_counts[trainers[j]]\n",
        "                    le.unregister(trainers[i])\n",
        "                    le.unregister(trainers[j])\n",
        "                except Exception as e:\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "\n",
        "            scores[i] += wins_player0\n",
        "            scores[j] += wins_player1\n",
        "            #print(f\"Match Results: {trainers[i].competitor.battle_policy.name} won {wins_player0}, {trainers[j].competitor.battle_policy.name} won {wins_player1}\")\n",
        "\n",
        "    # sorted ranking\n",
        "    ranking = sorted(zip(trainers, scores), key=lambda x: x[1], reverse=True)\n",
        "    print(\"\\nRanking tournament:\\n\")\n",
        "    for i, (trainer, score) in enumerate(ranking, start=1):\n",
        "        print(f\"{i}. {trainer.competitor.battle_policy.name} - {score} punti\")"
      ],
      "metadata": {
        "id": "kRLYWhIhIOl0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tournament\n",
        "\n",
        "We organized a round-robin tournament, in which each player fights against every other player and earns one point for each match won. Each confrontation consists of N matches, each of which is made up of 3 individual battles. The number of matches won is counted, and the player rankings are created. We show the results for the tournament with N = 10."
      ],
      "metadata": {
        "id": "iHJ_YKzFXlXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tournament()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyMoIk_ma5dz",
        "outputId": "6f8425fb-5630-4c75-c802-4f786f0c39a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let the Tournament begin\n",
            "Tournament\n",
            "\n",
            "Ranking tournament:\n",
            "\n",
            "1. My Policy - 36 punti\n",
            "2. My Minimax - 32 punti\n",
            "3. Minimax with pruning alpha beta killer - 30 punti\n",
            "4. Minimax with pruning alpha beta killer and my eval - 25 punti\n",
            "5. My Minimax with my eval - 22 punti\n",
            "6. Random Player - 5 punti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can from the result our policy wins the tournament with the most winned matches. Also Minimax and Minimax with alpha beta pruning and killer move heuristic perform very well and they beat the minimax with our evaluation function whuch consider also the power of a move of a pokemon. In this tournament the fact that the implementation of minimax with our evaluation function dosen't perform so well can be by the fact that the team assigned to the two player (Minimax and Minimax with my eval) were to favorable to the Minimax player (even if they are selected in a random way)"
      ],
      "metadata": {
        "id": "G8NC_KMDskvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The result from the the simulations of the battle was allineated with what we thought. In fact every player with a policy different from the random one was able to defeat the random player, not always with outstanding results but they culd beat the random player. We implemented an algorithm that wins a very high percentage of matches (almost 100%) against the random agent and, as seen from the tournament, achieves very satisfactory results against the other implemented algorithms.\n",
        "\n",
        "The difference in performance with the change of evaluation function in the other two algorithms does not seem to be very impactful, as the two functions do not have fundamental differences in their approach. The major difference is with MyPolicy, as can be seen from the tournament results.  \n",
        "\n",
        "We believe that MyPolicy performs better than the various Minimax algorithms in this context because, with few game variables and possible strategies, this allowed us to be very specific in the implementation of the algorithm, which adapts well to the very specific situations of the game.\n",
        "\n",
        "MyPolicy performs well even against some of the agents from the VGC competitions of 2023 and 2024, we have shown these results in Appendix.\n"
      ],
      "metadata": {
        "id": "szMZB4G-tZcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "We tested the performance of MyPolicy with challenges consisting of 50 epochs against 3 agents from the 2023 and 3 from 2024 competitions. Here are the results obtained:\n",
        "\n",
        "- MyPolicy vs MySubmissionMR-M.Ruppert: 38-12\n",
        "- MyPolicy vs vgc_weiyi_yen-Wei Yi Yen: 49-1\n",
        "- MyPolicy vs WiktorBukowski-Wiktor Bukowski: 35-15\n",
        "- MyPolicy vs campiao-Pedro Campião: 30-20\n",
        "- MyPolicy vs Bot4TeamBuildPolicy-Anja Ka: 23-27\n",
        "- MyPolicy vs MyPokemon-hgvbhjvcfg gh: 34-16"
      ],
      "metadata": {
        "id": "dVGiPtvx34UN"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}