{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaRagadini/Pokemon_AIF_24_25/blob/dev/pokemon-vgc-engine-master/example/Notebook_project_Pokemon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pokemon_Battle PokeBob team**\n",
        "\n",
        "The project focuses on the track proposed in the course, specifically the section related to competitions. Our team selected the task involving the simulation of a Pokémon battle between two teams, each composed of three Pokémon. The battle consists of three matches, with the first player to knock out all three Pokémon of the opposing player declared the winner of the match. The battle is considered concluded when a player wins at least two out of three matches.\n",
        "\n",
        "The objective of the project was to develop an AI agent capable of defeating a random player. This random player selects its Pokémon moves arbitrarily without any strategic logic. Both players operate under the same conditions, with their teams assigned randomly, ensuring that any advantages or disadvantages are also determined by chance\n",
        "In the following sections, we will explain our approach to solving the task and outline the methodology we adopted, along with the results we obtained.  (capire come far andare i codici su colab)\n"
      ],
      "metadata": {
        "id": "uSQ9kvX2QiD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone https://github.com/GemmaRagadini/Pokemon_AIF_24_25.git\n",
        "%cd Pokemon_AIF_24_25/pokemon-vgc-engine-master/example\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE_4RJnMlLfh",
        "outputId": "7e70631a-1793-49e6-db8b-9197cc5af3e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pokemon_AIF_24_25'...\n",
            "remote: Enumerating objects: 1858, done.\u001b[K\n",
            "remote: Counting objects: 100% (244/244), done.\u001b[K\n",
            "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "remote: Total 1858 (delta 190), reused 128 (delta 77), pack-reused 1614 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1858/1858), 21.61 MiB | 28.05 MiB/s, done.\n",
            "Resolving deltas: 100% (412/412), done.\n",
            "/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/example\n",
            "Example_BattleEcosystem.py\t  Example_PkmBattleEnv.py\n",
            "Example_ChampionshipEcosystem.py  Example_RemoteChampionshipEcosystem.py\n",
            "Example_Competitor.py\t\t  Example_RemoteCompetitor.py\n",
            "Example_GeneratePkmRoster.py\t  Example_Teambuild.py\n",
            "Example_Match.py\t\t  Example_TreeChampionship.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "2yvr8UnLOXeR",
        "outputId": "6c3fd63f-6fee-46aa-d627-2870624134e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vgc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2a6e9b6dce07>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from Example_BattleEcosystem import Tournament, main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mExample_BattleEcosystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/example/Example_BattleEcosystem.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mExample_Competitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExampleCompetitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardMetaData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompetitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompetitorManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mecosystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBattleEcosystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBattleEcosystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPkmRosterGenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomPkmRosterGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pokemon_AIF_24_25/pokemon-vgc-engine-master/example/Example_Competitor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBattlePolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTeamSelectionPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTeamBuildPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBattlePolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomPlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTerminalPlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTeamBuildPolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTerminalTeamBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomTeamBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehaviour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTeamSelectionPolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFirstEditionTeamSelectionPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompetitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompetitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vgc'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "#from Example_BattleEcosystem import Tournament, main\n",
        "from Example_BattleEcosystem.py import main\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Related Work**\n",
        "\n",
        "For this project, we decided to explore some existing approaches related to the concepts studied during the course, as well as to develop a custom approach of our own. These approaches were evaluated through a tournament, the details of which will be provided in the corresponding section.\n",
        "\n",
        "The related work was derived from the course slides and the accompanying textbook, Artificial Intelligence Fundamentals. Specifically, we focused on the section related to game theory (Chapter 6 of the textbook) and examined various approaches to the Minimax algorithm and its variations. This included the implementation of alpha-beta pruning and heuristics such as the killer move heuristic."
      ],
      "metadata": {
        "id": "czJQB6UsZVMv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jU6PcE_DhArV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methodology**\n",
        "\n",
        "To achieve our goal, we decided to implement several algorithms discussed in the related work. Initially, we focused our attention on various implementations of the Minimax algorithm. In the following sections, we will provide a detailed explanation of each algorithm we implemented."
      ],
      "metadata": {
        "id": "RLuFhPYL_ZQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minimax**\n",
        "\n",
        "The first implementation is the one obout a simple minimax with a simple evaluation function called game_eval. Here below there is our implementation.\n"
      ],
      "metadata": {
        "id": "71GjO_fpBQUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def game_state_eval(s: GameState, depth):\n",
        "    mine = s.teams[0].active\n",
        "    opp = s.teams[1].active\n",
        "    return mine.hp / mine.max_hp - 3 * opp.hp / opp.max_hp - 0.3 * depth\n",
        "\n",
        "\n",
        "def n_fainted(t: PkmTeam):\n",
        "    fainted = 0\n",
        "    fainted += t.active.hp == 0\n",
        "    if len(t.party) > 0:\n",
        "        fainted += t.party[0].hp == 0\n",
        "    if len(t.party) > 1:\n",
        "        fainted += t.party[1].hp == 0\n",
        "    return fainted\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyMinimax(BattlePolicy):\n",
        "\n",
        "    def __init__(self, max_depth: int = 4):\n",
        "        self.max_depth = max_depth\n",
        "        self.name = \"Minimax\"\n",
        "\n",
        "    def minimax(self, g, depth, is_maximizing_player):\n",
        "        \"\"\"\n",
        "        Classic Minimax with basic evaluation function.\n",
        "\n",
        "        :param g: current game state.\n",
        "        :param depth: the depth of the research.\n",
        "        :param is_maximizing_player: True if the player is a maximaxer or False if it is a minimazer.\n",
        "        :return: (valutazione, azione migliore)\n",
        "        \"\"\"\n",
        "        if depth == 0:\n",
        "            # the evaluation function is the basic evaluation function that evaluates the Hp of the pokemons.\n",
        "            return game_state_eval(g, depth), None\n",
        "\n",
        "        if is_maximizing_player:\n",
        "            max_eval = float('-inf')\n",
        "            best_action = None\n",
        "            for i in range(DEFAULT_N_ACTIONS):\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([i, 99])  # the enemy does not do a correct action the state does not change\n",
        "                if n_fainted(s[0].teams[0]) > n_fainted(g.teams[0]):\n",
        "                    continue # ignores the state where our pokemon loose.\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, False)\n",
        "                if eval_score > max_eval:\n",
        "                    max_eval = eval_score\n",
        "                    best_action = i\n",
        "            return max_eval, best_action\n",
        "\n",
        "        else:  # part of the enemy where he tries to minimize\n",
        "            min_eval = float('inf')\n",
        "            best_action = None\n",
        "            for j in range(DEFAULT_N_ACTIONS):\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([99, j])  # The player does not change the action (not valid action)\n",
        "                # it ignores the state where the defeated pokemon of the enemy increase.\n",
        "                if n_fainted(s[0].teams[1]) > n_fainted(g.teams[1]):\n",
        "                    continue\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, True)\n",
        "                if eval_score < min_eval:\n",
        "                    min_eval = eval_score\n",
        "                    best_action = j\n",
        "            return min_eval, best_action\n",
        "\n",
        "    def get_action(self, g) -> int:\n",
        "        \"\"\"\n",
        "        bets action to do by the maximazer player.\n",
        "\n",
        "        :param g: current game state.\n",
        "        :return: bets action to do.\n",
        "        \"\"\"\n",
        "        _, best_action = self.minimax(g, self.max_depth, True)\n",
        "        return best_action if best_action is not None else 0\n",
        "\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, state, parent, player, action=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.player = player\n",
        "        self.action = action\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.value = 0\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.get_untried_actions()) == 0\n",
        "\n",
        "    def get_untried_actions(self):\n",
        "        # Ottiene le azioni possibili dallo stato\n",
        "        return [i for i in range(DEFAULT_N_ACTIONS)]\n"
      ],
      "metadata": {
        "id": "WtNN0DTFC2Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell above, we present our first implementation of the Minimax policy, which employs a basic evaluation function called game_state_eval. This function is designed to:\n",
        "\n",
        "Encourage states where the player’s active Pokémon (mine) has higher HP relative to its maximum HP.\n",
        "Penalize states where the opponent’s active Pokémon (opp) has high HP.\n",
        "Add a penalty proportional to the search depth to prioritize faster victories.\n",
        "Although this evaluation function is rudimentary, our results demonstrate that it provides a balanced implementation. However, it is not the most effective approach we encountered.\n",
        "\n",
        "The Minimax implementation is straightforward, comprising a section for the maximizer player and another for the minimizer. The maximizer aims to transition to states where its Pokémon are healthier than the opponent’s Pokémon, while the minimizer seeks to reduce this advantage. Each state is evaluated recursively.\n",
        "\n",
        "To support these computations, the algorithm uses the n_fainted function, which counts the number of fainted (knocked-out) Pokémon. Additionally, the algorithm determines the next action from the maximizer player’s perspective, as implemented in the get_action method.\n",
        "\n",
        "Another critical function used is the step function, which simulates actions to predict how the game state evolves. This function works in conjunction with the n_fainted function to enhance decision-making.\n",
        "\n",
        "The default values for the search depth and weights in the evaluation function were determined empirically. Various configurations were tested, and the ones used here were found to deliver the best performance according to our evaluation metrics.\n",
        "\n",
        "The Node class, shown at the end of the cell, represents a node in the Minimax tree. It includes several fields to facilitate tree exploration, such as parent, children, value, and state.\n",
        "\n",
        "Additionally, the class provides two key methods:\n",
        "\n",
        "fully_expand: Checks whether the node has been fully explored.\n",
        "get_untried_actions: Retrieves the set of possible actions that can still be taken from the current node.\n",
        "These fields and methods are critical for efficiently navigating and expanding the Minimax tree during the decision-making process.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TX6rZ_AqEoUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minimax with Alpha-Beta Pruning and Killer Move Heuristic**\n",
        "\n",
        "Our second implementation extends the basic Minimax algorithm by incorporating alpha-beta pruning and the killer move heuristic. This implementation was developed to enhance both the performance and efficiency of the Minimax algorithm described above.\n",
        "\n",
        "The addition of alpha-beta pruning allows the algorithm to eliminate branches in the search tree that cannot influence the final decision, significantly reducing the number of nodes explored. Meanwhile, the killer move heuristic prioritizes moves that are likely to be effective, further optimizing the decision-making process by focusing on promising actions.\n",
        "\n",
        "The combined use of these techniques aims to not only improve the accuracy of the algorithm but also speed up its execution, enabling faster and more effective decision-making.\n",
        "\n",
        "In the cells below, we present our implementation of this enhanced Minimax algorithm along with a detailed explanation of how it operates."
      ],
      "metadata": {
        "id": "mk6j3adLRxU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMinimaxWithAlphaBetaKiller(BattlePolicy):\n",
        "\n",
        "    def __init__(self, max_depth: int = 5):\n",
        "        self.max_depth = max_depth\n",
        "        self.name = \"Minimax with pruning alpha beta killer\"\n",
        "        self.killer_moves = {depth: [] for depth in range(max_depth + 1)}  # Memorizza le killer moves per profondità\n",
        "\n",
        "    def minimax(self, g, depth, alpha, beta, is_maximizing_player):\n",
        "        if depth == 0:\n",
        "            return evalFunctions.game_state_eval(g, depth), None\n",
        "\n",
        "        if is_maximizing_player:\n",
        "            max_eval = float('-inf')\n",
        "            best_action = None\n",
        "\n",
        "            # Ottieni le azioni disponibili\n",
        "            moves = list(range(DEFAULT_N_ACTIONS))\n",
        "\n",
        "            # Prioritizza le killer moves\n",
        "            killer_moves = self.killer_moves.get(depth, [])\n",
        "            moves = sorted(moves, key=lambda move: move in killer_moves, reverse=True)\n",
        "\n",
        "            for i in moves:\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([i, 99])\n",
        "                if evalFunctions.n_fainted(s[0].teams[0]) > evalFunctions.n_fainted(g.teams[0]):\n",
        "                    continue\n",
        "\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, alpha, beta, False)\n",
        "                if eval_score > max_eval:\n",
        "                    max_eval = eval_score\n",
        "                    best_action = i\n",
        "\n",
        "                alpha = max(alpha, eval_score)\n",
        "                if beta <= alpha:\n",
        "                    # Aggiorna le killer moves\n",
        "                    if i not in self.killer_moves[depth]:\n",
        "                        self.killer_moves[depth].append(i)\n",
        "                        if len(self.killer_moves[depth]) > 2:\n",
        "                            self.killer_moves[depth].pop(0)\n",
        "                    break\n",
        "            return max_eval, best_action\n",
        "\n",
        "        else:\n",
        "            min_eval = float('inf')\n",
        "            best_action = None\n",
        "\n",
        "            # Ottieni le azioni disponibili\n",
        "            moves = list(range(DEFAULT_N_ACTIONS))\n",
        "\n",
        "            # Prioritizza le killer moves\n",
        "            killer_moves = self.killer_moves.get(depth, [])\n",
        "            moves = sorted(moves, key=lambda move: move in killer_moves, reverse=True)\n",
        "\n",
        "            for j in moves:\n",
        "                g_copy = deepcopy(g)\n",
        "                s, _, _, _, _ = g_copy.step([99, j])\n",
        "                if evalFunctions.n_fainted(s[0].teams[1]) > evalFunctions.n_fainted(g.teams[1]):\n",
        "                    continue\n",
        "\n",
        "                eval_score, _ = self.minimax(s[0], depth - 1, alpha, beta, True)\n",
        "                if eval_score < min_eval:\n",
        "                    min_eval = eval_score\n",
        "                    best_action = j\n",
        "\n",
        "                beta = min(beta, eval_score)\n",
        "                if beta <= alpha:\n",
        "                    # Aggiorna le killer moves\n",
        "                    if j not in self.killer_moves[depth]:\n",
        "                        self.killer_moves[depth].append(j)\n",
        "                        if len(self.killer_moves[depth]) > 2:\n",
        "                            self.killer_moves[depth].pop(0)\n",
        "                    break\n",
        "            return min_eval, best_action\n",
        "\n",
        "    def get_action(self, g) -> int:\n",
        "        _, best_action = self.minimax(g, self.max_depth, float('-inf'), float('inf'), True)\n",
        "        return best_action if best_action is not None else 0"
      ],
      "metadata": {
        "id": "nRhf_-spRwrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section details the implementation of an advanced Minimax algorithm enhanced with Alpha-Beta pruning and the Killer Moves heuristic. The algorithm optimizes decision-making in competitive game scenarios by efficiently evaluating possible actions and pruning less relevant game tree branches.\n",
        "\n",
        "Overview\n",
        "\n",
        "The Minimax algorithm aims to find the best possible action for a player in a turn-based game by exploring potential future states. This implementation improves the basic Minimax algorithm with two key techniques:\n",
        "\n",
        "Alpha-Beta Pruning: Reduces the number of nodes evaluated by eliminating branches that cannot affect the final decision.\n",
        "\n",
        "Killer Moves Heuristic: Prioritizes actions (\"killer moves\") that have previously led to significant cutoffs, increasing the likelihood of early pruning.\n",
        "\n",
        "Attributes\n",
        "\n",
        "max_depth: Specifies the maximum depth of the search tree.\n",
        "\n",
        "name: Identifies the policy as \"Minimax with pruning alpha beta killer.\"\n",
        "\n",
        "killer_moves: A dictionary mapping depths to lists of \"killer moves.\" Each depth can store up to two killer moves that caused Beta cutoffs.\n",
        "\n",
        "(da finire)"
      ],
      "metadata": {
        "id": "i8hRaI0RkecE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Evaluation Function**\n",
        "\n",
        "We decided to implement an other evaluation function that differs in the approch respect to the one described above. This new evaluation function is a more aggressive evaluation function in fact take into consideration only the power and the effectivness of a move against the opponent. The aim of this new evaluation function was to create a more aggresive agent so the battles could and in more rapid way. *The result obtained are illustrated and commented in the Evaluation section*."
      ],
      "metadata": {
        "id": "Wh7c3eUhqjAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Policy**\n",
        "\n",
        "The third and last approch was the one about a our custom Policy. This policy differ form the other that has been implemented"
      ],
      "metadata": {
        "id": "dYhf6x63nQWK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mO64wj0PnP9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descrizione della custom Policy"
      ],
      "metadata": {
        "id": "Z1VhZfhmqNn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**\n",
        "\n",
        "We implemented several approaches and, in order to determine which policy performed best, we decided to have each agent battle against the Random agent. This allowed us to demonstrate that each policy outperforms the Random agent.\n",
        "\n",
        "For the second evaluation, we organized a tournament involving all the players:\n",
        "\n",
        "Random Player\n",
        "\n",
        "Minimax Player\n",
        "\n",
        "Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player\n",
        "\n",
        "Custom Policy Player\n",
        "\n",
        "Each match consisted of 10 battles, and the winner was determined by the player who won the most matches, thus achieving the best win rate.\n",
        "\n",
        "The results of each individual battle, as well as the overall winner of the tournament, are reported in the cells below."
      ],
      "metadata": {
        "id": "M43Al93JqRyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Battle: Minimax vs Random Player: the first result is when the two player have always a different team and the second when they have always the same team"
      ],
      "metadata": {
        "id": "xhi3IXEHsuPj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6MPOG1Nds0pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Piccolo commento sui risultati"
      ],
      "metadata": {
        "id": "G8NC_KMDskvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second Battle: Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player vs Random Player. The first result is when the two player have always a different team and the second when they have always the same team"
      ],
      "metadata": {
        "id": "E71rP0Pis1gu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Ib_CBagtE-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "piccolo commento sui risultati"
      ],
      "metadata": {
        "id": "wL5eVE_XsiQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third Battle : Custom Player vs Random Player. The first result is when the two player have always a different team and the second when they have always the same team"
      ],
      "metadata": {
        "id": "53DgqD1EtGNm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n11v0WDgtR8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "piccolo commento sul risultato"
      ],
      "metadata": {
        "id": "Wpo7Urj7sen4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuornament between all the player involved. The first result is when the two player have always a different team and the second when they have always the same team"
      ],
      "metadata": {
        "id": "B28q4sbFtPtd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNQTd5OntQje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Piccolo commento sul risultato"
      ],
      "metadata": {
        "id": "NIluu3wzsbgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "The result from the the simulations of the battle was allineated with what we thought. In fact every player with a policy different from the random one was able to defeat the random playeer, not always with outstanding results but they culd beat the random player. Our new evaluation function apllied to the minimax and to the other minimax with the alpha beta pruning and killer move heuristic function aimed to obtain a more aggressive player more focused on the attack fase and on the power of the moves in the roster. At the beginning we thougth that this could be the better approch and that battle would be ended in a very fast way. This was partialy true beacuse it was only partially more faster but the win rate was not as good as we fought maybe a more conservative approch wuolb be better. The best out of the 4 Player was the one with the custom policy. This because the custom policy takes into consideration variuos aspect of the game for example take into consideration when to do a switch between our pokemon and with whom to do this switch, so the policy was aggressive becauese aimed to search the best and powerful move but take into account also the matchup with the pokemon of the opponent, giving to the Custom policy a better knowledge of the state of the game (da rivedere)"
      ],
      "metadata": {
        "id": "szMZB4G-tZcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appendix**\n",
        "(se vogliamo metterci qualcosa ma vediamo dopo)"
      ],
      "metadata": {
        "id": "LbjeRHRbsR9t"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}