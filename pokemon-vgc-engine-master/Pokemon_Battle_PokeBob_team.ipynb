{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaRagadini/Pokemon_AIF_24_25/blob/main/pokemon-vgc-engine-master/Pokemon_Battle_PokeBob_team.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PokeBoB Pokemon Battle Track\n",
        "\n",
        "**team**: **Pokebob**\n",
        "\n",
        "**members**:  **Gemma Ragadini** and **Filippo Alessandro Sandoval Villarreal**"
      ],
      "metadata": {
        "id": "de0sXQYdRhJ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSQ9kvX2QiD0"
      },
      "source": [
        "The project focuses on the track proposed in the course, specifically the section related to competitions. We selected the task involving the simulation of a Pokémon battle between two teams, each composed of three Pokémon. The battle consists of three matches, with the first player to knock out all three Pokémon of the opposing player declared the winner of the match. The battle is considered concluded when a player wins at least two out of three matches.\n",
        "The objective of the project was to develop a competitive AI agent in the Pokémon battle environment. We started by challenging a random player who selects its Pokémon moves arbitrarily without any strategic logic. Both players operate under the same conditions, with their teams assigned randomly and regenerated before each challenge, ensuring that any advantages or disadvantages are also determined by chance.\n",
        "In the following sections, we will explain our approach to solving the task and outline the methodology we adopted, along with the results we obtained.\n",
        "\n",
        "The repository is available at this link: https://github.com/GemmaRagadini/Pokemon_AIF_24_25.git\n",
        "\n",
        "This Notebook should be runned once the repository is clooned and it shuold be runned within the principal directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7AhyiJjZjDJ"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DHAISC0tzu5"
      },
      "outputs": [],
      "source": [
        "!python3 -m venv amb\n",
        "!source amb/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czJQB6UsZVMv"
      },
      "source": [
        "**Related Work**\n",
        "\n",
        "For this project, we decided to explore some existing approaches related to the concepts studied during the course, as well as to develop a custom approach of our own. These approaches were evaluated through a tournament, the details of which will be provided in the corresponding section (Evaluation).\n",
        "\n",
        "The related work was derived from the course slides and the accompanying textbook, *Artificial Intelligence A Modern Approach - Fourth Edition. Stuart Russer, Peter Norvig*. Specifically, we focused on the section related to game theory (Chapter 6 of the textbook) and examined various approaches to the Minimax algorithm and its variations. This included the implementation of alpha-beta pruning and heuristics such as the killer move heuristic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLuFhPYL_ZQm"
      },
      "source": [
        "**Methodology**\n",
        "\n",
        "To achieve our goal, we decided to implement several algorithms discussed in the related work (section above). We focused our attention on various implementations of the Minimax algorithm using 2 different evaluation functions. In the following sections, we will provide a detailed explanation of each algorithm we implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRgFMfeRHLzC"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71GjO_fpBQUg"
      },
      "source": [
        "**Implemented Policies**\n",
        "\n",
        "We implemented 3 algorithms: classic Minimax, Minimax with alpha-beta pruning and killer heuristic, and My Policy. The third one is the policy that performed the best. The first two were tested with two different evaluation functions, `game_state_eval` and `my_eval_fun`.\n",
        "\n",
        "The two Minimax algorithms are located in the file `vgc/behaviour/otherPolicies.py`, while `MyPolicy` is located in the file `vgc/behaviour/myPolicy.py`.\n",
        "\n",
        "\n",
        "**Minimax**\n",
        "\n",
        "The first implementation is the one obout a classic minimax algorithm with a pre-existent evaluation function called `game_state_eval` (given by the project base line). The Minimax's policy evaluation functions `game_state_eval` or `my_eval_fun` are described in the *Evaluation Functions* section.\n",
        "The Minimax implementation is straightforward, comprising a section for the maximizer player and another for the minimizer. The maximizer aims to transition to states where its Pokémon are healthier than the opponent’s Pokémon, while the minimizer seeks to reduce this advantage (according to the evaluation function). Each state is evaluated recursively.\n",
        "To support these computations, the algorithm uses the, not only the `game_state_eval`, but also the `n_defeated` function (in `vgc/behaviour/evalFunctions.py`), which counts the number of fainted (knocked-out) Pokémon. Additionally, the algorithm determines the next action from the maximizer player’s perspective, as implemented in the `get_action` method.The default values for the search depth and weights in the evaluation function were determined empirically. Various configurations were tested, and the ones we used were found to deliver the best performance according to our evaluation metrics. A detailed version of the algorithm could be found in the `vgc/behaviour/otherPolicies.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk6j3adLRxU7"
      },
      "source": [
        "**Minimax with Alpha-Beta Pruning and Killer Move Heuristic**\n",
        "\n",
        "Our second implementation extends the basic Minimax algorithm by incorporating alpha-beta pruning and the killer move heuristic. This implementation was developed to enhance both the performance and efficiency of the Minimax algorithm described above.\n",
        "The addition of alpha-beta pruning allows the algorithm to eliminate branches in the search tree that cannot influence the final decision, significantly reducing the number of nodes explored. Meanwhile, the killer move heuristic prioritizes moves that are likely to be effective, further optimizing the decision-making process by focusing on promising actions.\n",
        "The combined use of these techniques aims to not only improve the accuracy of the algorithm but also speed up its execution, enabling faster and more effective decision-making. This algorithm works almost the same as Minimax except for the fact that Alpha-Beta and the Killer. Also this algorithm was implemented with the two evaluation functions `game_state_eval` and `my_eval_fun`.  A detailed version of the algorithm could be found in the `vgc/behaviour/otherPolicies.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJYj_LsyVqU"
      },
      "source": [
        "**Evaluation Functions**\n",
        "\n",
        "The evaluation function plays a central role in determining the quality of a game state during a Pokémon battle simulation.\n",
        "The purpose of the evaluation function is to return a numerical score that represents the desirability of the current game state. Higher scores indicate more favorable conditions for the agent, while lower scores highlight disadvantages.\n",
        "\n",
        "`game_state_eval` is the pre-existent evaluation function, it encourages states where the player’s active Pokémon (mine) has higher HP relative to its maximum HP and penalizes states where the opponent’s active Pokémon (opp) has high HP.\n",
        "Finally it adds a penalty proportional to the search depth to prioritize faster victories.\n",
        "Our results demonstrate that it provides a balanced implementation. However, it is not the most effective approach we encountered.\n",
        "To try to improve the performance of the algorithms, we implemented another evaluation function, `my_eval_fun` that builds upon the previous one.\n",
        "The function achieves this by evaluating three core aspects: Type Compatibility, Health Points (HP) Analysis and Damage Potential.\n",
        "These values are combined into a single score using weighted contributions.It is a balanced evaluation function that considers not only the Pokémon's health points but also the potential damage they can inflict on the opponent.\n",
        "\n",
        "A detailed version of the algorithm could be found in the `vgc/behaviour/evalFunctions.py`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYhf6x63nQWK"
      },
      "source": [
        "**Custom Policy**\n",
        "\n",
        "The third and last approch was the one about a our custom Policy, implemented in `behaviour/myPolicy.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO64wj0PnP9M"
      },
      "outputs": [],
      "source": [
        "class MyPolicy(BattlePolicy):\n",
        "    def __init__(self):\n",
        "        self.hail_used = False\n",
        "        self.sandstorm_used = False\n",
        "        self.name = \"My Policy\"\n",
        "    def assess_damages(self, active_pkm: Pkm, opp_pkm_type: PkmType, attack_stage: int, defense_stage: int, weather: WeatherCondition)-> int:\n",
        "        # moves evaluation\n",
        "        damages: List[float] = []\n",
        "        for move in active_pkm.moves:\n",
        "            damages.append(evalFunctions.estimate_damage(move.type, active_pkm.type, move.power, opp_pkm_type, attack_stage,defense_stage, weather))\n",
        "        return damages\n",
        "    def get_action(self, g: GameState) -> int:\n",
        "        # my team\n",
        "        my_team = g.teams[0]\n",
        "        active_pkm = my_team.active\n",
        "        bench = my_team.party\n",
        "        my_attack_stage = my_team.stage[PkmStat.ATTACK]\n",
        "        # opposite team\n",
        "        opp_team = g.teams[1]\n",
        "        opp_active_pkm = opp_team.active\n",
        "        opp_defense_stage = opp_team.stage[PkmStat.DEFENSE]\n",
        "        # weather\n",
        "        weather = g.weather.condition\n",
        "        try:\n",
        "            # estimate of the damage of each move\n",
        "            damages = self.assess_damages(active_pkm, opp_active_pkm.type, my_attack_stage, opp_defense_stage, weather)\n",
        "            move_id = int(np.argmax(damages))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        # if it eliminates the opponent or the move type is super effective, use it immediately\n",
        "        if (damages[move_id] >= opp_active_pkm.hp) or (damages[move_id] > 0 and TYPE_CHART_MULTIPLIER[active_pkm.moves[move_id].type][opp_active_pkm.type] == 2.0) :\n",
        "            return move_id\n",
        "        try:\n",
        "            defense_type_multiplier = evalFunctions.examine_matchup(active_pkm.type, opp_active_pkm.type, list(map(lambda m: m.type, opp_active_pkm.moves)))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        if defense_type_multiplier <= 1.0:\n",
        "            return move_id\n",
        "        # Consider the Pokémon switch\n",
        "        matchup: List[float] = []\n",
        "        not_fainted = False\n",
        "        try:\n",
        "            for j in range(len(bench)):\n",
        "                if bench[j].hp == 0.0:\n",
        "                    matchup.append(0.0)\n",
        "                else:\n",
        "                    not_fainted = True\n",
        "                    matchup.append(\n",
        "                        evalFunctions.examine_matchup(bench[j].type, opp_active_pkm.type, list(map(lambda m: m.type, bench[j].moves))))\n",
        "            best_switch_matchup = int(np.max(matchup))\n",
        "            best_switch = np.argmax(matchup)\n",
        "            current_matchup = evalFunctions.examine_matchup(active_pkm.type, opp_active_pkm.type,list(map(lambda m: m.type, active_pkm.moves)))\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        if not_fainted and best_switch_matchup >= current_matchup+1:\n",
        "            return best_switch + 4\n",
        "        return move_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1VhZfhmqNn6"
      },
      "source": [
        "The policy is designed to make decisions about the actions the player's team should take based on an evaluation of damage, the Pokémon’s types, and various other battle conditions, such as weather. Here's an overview of its functionality:\n",
        "\n",
        "*Initialization*: The class initializes two flags, hail_used and sandstorm_used, to track if certain weather conditions have already been activated during the battle.\n",
        "\n",
        "*Damage Assessment* (`assess_damages`):\n",
        "This method evaluates the potential damage of each move available to the active Pokémon, using `evalFunctions.estimate_damage`. It calculates the damage based on various factors such as:\n",
        "\n",
        "  - The Pokémon's attack and the opponent's defense stages.\n",
        "  - The Pokémon types and move types.\n",
        "  - Weather conditions that might affect damage output.\n",
        "\n",
        "It then returns a list of damage estimates for all available moves.\n",
        "\n",
        "*Action Selection* (`get_action`):\n",
        "This is the primary method used to decide the next action. It follows a series of steps to make the decision:\n",
        "\n",
        "  - Team Setup: It first extracts the active Pokémon from the player's team and the opponent's team.\n",
        "  - Weather Condition: It retrieves the current weather condition, which can influence the effectiveness of moves.\n",
        "  - Damage Calculation: Using the `assess_damages` method, it calculates the potential damage for each move and selects the move with the highest estimated damage.\n",
        "\n",
        "*Move Selection Logic*: If a move will eliminate the opponent or if it is \"super effective\" (based on the type chart), it is selected immediately. If the active Pokémon is at a disadvantage in terms of move effectiveness, the policy evaluates to switch to a Pokémon from the bench (the reserve Pokémon) that has a better matchup against the opponent's active Pokémon. This is determined by evaluating the compatibility between each Pokémon on the bench and the opponent’s active Pokémon. The policy uses a comparison between the matchups of the active Pokémon and each bench Pokémon, selecting the Pokémon that has a significantly better type advantage.\n",
        "\n",
        "*Pokémon Switch Consideration*: If there is at least one Pokémon on the bench that has a favorable matchup compared to the active Pokémon, the policy will switch to that Pokémon. This is done by comparing the matchup values, and if the bench Pokémon’s matchup score is sufficiently better (greater than the current active Pokémon's matchup by at least 1), it will choose to switch to that Pokémon. When switching Pokémon, \"time\" is lost in battle, so the policy chooses to do so only if the gain is considerable.\n",
        "\n",
        "The policy uses the `evaluate_matchup` function to assess the compatibility between Pokémon types based on their moves, making it a dynamic policy that adapts to the strengths and weaknesses of the opposing team.\n",
        "\n",
        "This battle policy uses a combination of damage estimation, type advantages, and Pokémon switching strategies to make optimal decisions, ensuring that the player can both maximize damage and strategically manage their team for better performance in battle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M43Al93JqRyH"
      },
      "source": [
        "**Performance Evaluation**\n",
        "\n",
        "We implemented several approaches and first thing we decided to have each agent battle against the Random agent. This allowed us to demonstrate that each policy outperforms the Random agent.\n",
        "We chose to use a random team for each player, generated for every match, so that with a large number of executions, the influence of the chosen team on the evaluation of the algorithm's performance decreases. *In the code below, the functions `SingleCombat` and `Tournament`, located in the `utils.py` file, are called due to space constraints.*\n",
        "\n",
        "For the second evaluation, we organized a tournament involving these players:\n",
        "\n",
        "- *My Policy Player*\n",
        "- *Random Player*\n",
        "- *Minimax Player*\n",
        "- *Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player*\n",
        "- *Minimax Player using `my_eval_fun`*\n",
        "- *Minimax with Alpha-Beta Pruning and Killer Move Heuristic Player using `my_eval_fun`*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRLYWhIhIOl0"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    turnament=sys.argv[-1]\n",
        "    if turnament== 't':\n",
        "        utils.Tournament()\n",
        "    else:\n",
        "        utils.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHJ_YKzFXlXf"
      },
      "source": [
        "**Tournament**\n",
        "\n",
        "We organized a round-robin tournament, in which each player fights against every other player and earns one point for each match won. Each confrontation consists of N matches, each of which is made up of 3 individual battles. The number of matches won is counted, and the player rankings are created. We show the results for the tournament with N = 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyMoIk_ma5dz"
      },
      "outputs": [],
      "source": [
        "utils.Tournament()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_IgZZMWHLzE"
      },
      "source": [
        "This is the result of a previous execution in textual form, as the tournament's execution time is approximately 3 hours:\n",
        "- My Policy - 40 punti  \n",
        "- My Minimax - 28 punti  \n",
        "- My Minimax with my eval - 26 punti  \n",
        "- Minimax with pruning alpha beta killer - 24 punti  \n",
        "- Minimax with pruning alpha beta killer and my eval - 24 punti  \n",
        "- Random Player - 8 punti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8NC_KMDskvF"
      },
      "source": [
        "As we can from the result our policy wins the tournament with the most winned matches. Also Minimax and Minimax with alpha beta pruning and killer move heuristic perform very well and they beat the minimax with our evaluation function whuch consider also the power of a move of a pokemon. In this tournament the fact that the implementation of minimax with our evaluation function dosen't perform so well can be by the fact that the team assigned to the two player (Minimax and Minimax with my eval) were to favorable to the Minimax player (even if they are selected in a random way)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMZB4G-tZcm"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "The result from the the simulations of the battle was allineated with what we thought. In fact every player with a policy different from the random one was able to defeat the random player, not always with outstanding results but they culd beat the random player. We implemented an algorithm that wins a very high percentage of matches (almost 100%) against the random agent and, as seen from the tournament, achieves very satisfactory results against the other implemented algorithms.\n",
        "The difference in performance with the change of evaluation function in the other two algorithms does not seem to be very impactful, as the two functions do not have fundamental differences in their approach. The major difference is with MyPolicy, as can be seen from the tournament results.  \n",
        "We believe that MyPolicy performs better than the various Minimax algorithms in this context because, with few game variables and possible strategies, this allowed us to be very specific in the implementation of the algorithm, which adapts well to the very specific situations of the game.\n",
        "MyPolicy performs well even against some of the agents from the VGC competitions of 2023 and 2024, we have shown these results in Appendix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVGiPtvx34UN"
      },
      "source": [
        "**Appendix**\n",
        "\n",
        "We tested the performance of MyPolicy with challenges consisting of 50 epochs against 3 agents from the 2023 and 3 from 2024 competitions. Here are the results obtained:\n",
        "\n",
        "- MyPolicy vs MySubmissionMR-M.Ruppert: 38-12\n",
        "- MyPolicy vs vgc_weiyi_yen-Wei Yi Yen: 49-1\n",
        "- MyPolicy vs WiktorBukowski-Wiktor Bukowski: 35-15\n",
        "- MyPolicy vs campiao-Pedro Campião: 30-20\n",
        "- MyPolicy vs Bot4TeamBuildPolicy-Anja Ka: 23-27\n",
        "- MyPolicy vs MyPokemon-hgvbhjvcfg gh: 34-16"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "amb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}